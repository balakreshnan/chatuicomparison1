<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Acs.CL%20AND%20submittedDate%3A%5B20240604%20TO%2020240611%5D%26id_list%3D%26start%3D0%26max_results%3D100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:cs.CL AND submittedDate:[20240604 TO 20240611]&amp;id_list=&amp;start=0&amp;max_results=100</title>
  <id>http://arxiv.org/api/Jw8dWKy03gkeRCzbEzGZe4MQ0Ms</id>
  <updated>2024-06-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">468</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2406.01860v1</id>
    <updated>2024-06-04T00:09:43Z</updated>
    <published>2024-06-04T00:09:43Z</published>
    <title>Eliciting the Priors of Large Language Models using Iterated In-Context
  Learning</title>
    <summary>  As Large Language Models (LLMs) are increasingly deployed in real-world
settings, understanding the knowledge they implicitly use when making decisions
is critical. One way to capture this knowledge is in the form of Bayesian prior
distributions. We develop a prompt-based workflow for eliciting prior
distributions from LLMs. Our approach is based on iterated learning, a Markov
chain Monte Carlo method in which successive inferences are chained in a way
that supports sampling from the prior distribution. We validated our method in
settings where iterated learning has previously been used to estimate the
priors of human participants -- causal learning, proportion estimation, and
predicting everyday quantities. We found that priors elicited from GPT-4
qualitatively align with human priors in these settings. We then used the same
method to elicit priors from GPT-4 for a variety of speculative events, such as
the timing of the development of superhuman AI.
</summary>
    <author>
      <name>Jian-Qiao Zhu</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <link href="http://arxiv.org/abs/2406.01860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01863v1</id>
    <updated>2024-06-04T00:30:37Z</updated>
    <published>2024-06-04T00:30:37Z</published>
    <title>Towards Effective Time-Aware Language Representation: Exploring Enhanced
  Temporal Understanding in Language Models</title>
    <summary>  In the evolving field of Natural Language Processing, understanding the
temporal context of text is increasingly crucial. This study investigates
methods to incorporate temporal information during pre-training, aiming to
achieve effective time-aware language representation for improved performance
on time-related tasks. In contrast to common pre-trained models like BERT,
which rely on synchronic document collections such as BookCorpus and Wikipedia,
our research introduces BiTimeBERT 2.0, a novel language model pre-trained on a
temporal news article collection. BiTimeBERT 2.0 utilizes this temporal news
collection, focusing on three innovative pre-training objectives: Time-Aware
Masked Language Modeling (TAMLM), Document Dating (DD), and Time-Sensitive
Entity Replacement (TSER). Each objective targets a unique aspect of temporal
information. TAMLM is designed to enhance the understanding of temporal
contexts and relations, DD integrates document timestamps as chronological
markers, and TSER focuses on the temporal dynamics of "Person" entities,
recognizing their inherent temporal significance. The experimental results
consistently demonstrate that BiTimeBERT 2.0 outperforms models like BERT and
other existing pre-trained models, achieving substantial gains across a variety
of downstream NLP tasks and applications where time plays a pivotal role.
</summary>
    <author>
      <name>Jiexin Wang</name>
    </author>
    <author>
      <name>Adam Jatowt</name>
    </author>
    <author>
      <name>Yi Cai</name>
    </author>
    <link href="http://arxiv.org/abs/2406.01863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01879v1</id>
    <updated>2024-06-04T01:20:14Z</updated>
    <published>2024-06-04T01:20:14Z</published>
    <title>Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework
  for Chinese Spelling Check</title>
    <summary>  Chinese Spelling Check (CSC) aims to detect and correct potentially
misspelled characters in Chinese sentences. Naturally, it involves the
detection and correction subtasks, which interact with each other dynamically.
Such interactions are bi-directional, i.e., the detection result would help
reduce the risk of over-correction and under-correction while the knowledge
learnt from correction would help prevent false detection. Current CSC
approaches are of two types: correction-only or single-directional
detection-to-correction interactive frameworks. Nonetheless, they overlook the
bi-directional interactions between detection and correction. This paper aims
to fill the gap by proposing a Bi-directional Detector-Corrector framework for
CSC (Bi-DCSpell). Notably, Bi-DCSpell contains separate detection and
correction encoders, followed by a novel interactive learning module
facilitating bi-directional feature interactions between detection and
correction to improve each other's representation learning. Extensive
experimental results demonstrate a robust correction performance of Bi-DCSpell
on widely used benchmarking datasets while possessing a satisfactory detection
ability.
</summary>
    <author>
      <name>Haiming Wu</name>
    </author>
    <author>
      <name>Hanqing Zhang</name>
    </author>
    <author>
      <name>Richeng Xuan</name>
    </author>
    <author>
      <name>Dawei Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.01879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01919v1</id>
    <updated>2024-06-04T03:00:55Z</updated>
    <published>2024-06-04T03:00:55Z</published>
    <title>OTTAWA: Optimal TransporT Adaptive Word Aligner for Hallucination and
  Omission Translation Errors Detection</title>
    <summary>  Recently, there has been considerable attention on detecting hallucinations
and omissions in Machine Translation (MT) systems. The two dominant approaches
to tackle this task involve analyzing the MT system's internal states or
relying on the output of external tools, such as sentence similarity or MT
quality estimators. In this work, we introduce OTTAWA, a novel Optimal
Transport (OT)-based word aligner specifically designed to enhance the
detection of hallucinations and omissions in MT systems. Our approach
explicitly models the missing alignments by introducing a "null" vector, for
which we propose a novel one-side constrained OT setting to allow an adaptive
null alignment. Our approach yields competitive results compared to
state-of-the-art methods across 18 language pairs on the HalOmi benchmark. In
addition, it shows promising features, such as the ability to distinguish
between both error types and perform word-level detection without accessing the
MT system's internal states.
</summary>
    <author>
      <name>Chenyang Huang</name>
    </author>
    <author>
      <name>Abbas Ghaddar</name>
    </author>
    <author>
      <name>Ivan Kobyzev</name>
    </author>
    <author>
      <name>Mehdi Rezagholizadeh</name>
    </author>
    <author>
      <name>Osmar R. Zaiane</name>
    </author>
    <author>
      <name>Boxing Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2024 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.01919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01931v2</id>
    <updated>2024-06-05T07:21:19Z</updated>
    <published>2024-06-04T03:31:09Z</published>
    <title>Dishonesty in Helpful and Harmless Alignment</title>
    <summary>  People tell lies when seeking rewards. Large language models (LLMs) are
aligned to human values with reinforcement learning where they get rewards if
they satisfy human preference. We find that this also induces dishonesty in
helpful and harmless alignment where LLMs tell lies in generating harmless
responses. Using the latest interpreting tools, we detect dishonesty, show how
LLMs can be harmful if their honesty is increased, and analyze such conflicts
at the parameter-level. Given these preliminaries and the hypothesis that
reward-seeking stimulates dishonesty, we theoretically show that the dishonesty
can in-turn decrease the alignment performances and augment reward-seeking
alignment with representation regularization. Extensive results, including
GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we
can train more honest, helpful, and harmless LLMs. We will make all our codes
and results be open-sourced upon this paper's acceptance.
</summary>
    <author>
      <name>Youcheng Huang</name>
    </author>
    <author>
      <name>Jingkun Tang</name>
    </author>
    <author>
      <name>Duanyu Feng</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <author>
      <name>Wenqiang Lei</name>
    </author>
    <author>
      <name>Jiancheng Lv</name>
    </author>
    <author>
      <name>Anthony G. Cohn</name>
    </author>
    <link href="http://arxiv.org/abs/2406.01931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01934v2</id>
    <updated>2024-06-05T12:13:56Z</updated>
    <published>2024-06-04T03:35:25Z</published>
    <title>Optimal Transport Guided Correlation Assignment for Multimodal Entity
  Linking</title>
    <summary>  Multimodal Entity Linking (MEL) aims to link ambiguous mentions in multimodal
contexts to entities in a multimodal knowledge graph. A pivotal challenge is to
fully leverage multi-element correlations between mentions and entities to
bridge modality gap and enable fine-grained semantic matching. Existing methods
attempt several local correlative mechanisms, relying heavily on the
automatically learned attention weights, which may over-concentrate on partial
correlations. To mitigate this issue, we formulate the correlation assignment
problem as an optimal transport (OT) problem, and propose a novel MEL
framework, namely OT-MEL, with OT-guided correlation assignment. Thereby, we
exploit the correlation between multimodal features to enhance multimodal
fusion, and the correlation between mentions and entities to enhance
fine-grained matching. To accelerate model prediction, we further leverage
knowledge distillation to transfer OT assignment knowledge to attention
mechanism. Experimental results show that our model significantly outperforms
previous state-of-the-art baselines and confirm the effectiveness of the
OT-guided correlation assignment.
</summary>
    <author>
      <name>Zefeng Zhang</name>
    </author>
    <author>
      <name>Jiawei Sheng</name>
    </author>
    <author>
      <name>Chuang Zhang</name>
    </author>
    <author>
      <name>Yunzhi Liang</name>
    </author>
    <author>
      <name>Wenyuan Zhang</name>
    </author>
    <author>
      <name>Siqi Wang</name>
    </author>
    <author>
      <name>Tingwen Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.01934v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01934v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01976v1</id>
    <updated>2024-06-04T05:22:24Z</updated>
    <published>2024-06-04T05:22:24Z</published>
    <title>Conditional Language Learning with Context</title>
    <summary>  Language models can learn sophisticated language understanding skills from
fitting raw text. They also unselectively learn useless corpus statistics and
biases, especially during finetuning on domain-specific corpora. In this paper,
we propose a simple modification to causal language modeling called conditional
finetuning, which performs language modeling conditioned on a context. We show
that a context can "explain away" certain corpus statistics and make the model
avoid learning them. In this fashion, conditional finetuning achieves selective
learning from a corpus, learning knowledge useful for downstream tasks while
avoiding learning useless corpus statistics like topic biases. This selective
learning effect leads to less forgetting and better stability-plasticity
tradeoff in domain finetuning, potentially benefitting lifelong learning with
language models.
</summary>
    <author>
      <name>Xiao Zhang</name>
    </author>
    <author>
      <name>Miao Li</name>
    </author>
    <author>
      <name>Ji Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 41st International Conference on Machine Learning
  (ICML 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.01976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01983v1</id>
    <updated>2024-06-04T05:51:43Z</updated>
    <published>2024-06-04T05:51:43Z</published>
    <title>RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning
  Personal Information in Large Language Models</title>
    <summary>  With the passage of the Right to Be Forgotten (RTBF) regulations and the
scaling up of language model training datasets, research on model unlearning in
large language models (LLMs) has become more crucial. Before the era of LLMs,
machine unlearning research focused mainly on classification tasks in models
with small parameters. In these tasks, the content to be forgotten or retained
is clear and straightforward. However, as parameter sizes have grown and tasks
have become more complex, balancing forget quality and model utility has become
more challenging, especially in scenarios involving personal data instead of
classification results. Existing methods based on gradient ascent and its
variants often struggle with this balance, leading to unintended information
loss or partial forgetting. To address this challenge, we propose RKLD, a novel
\textbf{R}everse \textbf{KL}-Divergence-based Knowledge \textbf{D}istillation
unlearning algorithm for LLMs targeting the unlearning of personal information.
Through RKLD, we achieve significant forget quality and effectively maintain
the model utility in our experiments.
</summary>
    <author>
      <name>Bichen Wang</name>
    </author>
    <author>
      <name>Yuzhe Zi</name>
    </author>
    <author>
      <name>Yixin Sun</name>
    </author>
    <author>
      <name>Yanyan Zhao</name>
    </author>
    <author>
      <name>Bing Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work is in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.01983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02050v2</id>
    <updated>2024-06-05T07:56:11Z</updated>
    <published>2024-06-04T07:31:06Z</published>
    <title>Analyzing Social Biases in Japanese Large Language Models</title>
    <summary>  With the development of Large Language Models (LLMs), social biases in the
LLMs have become a crucial issue. While various benchmarks for social biases
have been provided across languages, the extent to which Japanese LLMs exhibit
social biases has not been fully investigated. In this study, we construct the
Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the
English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The
results show that while current Japanese LLMs improve their accuracies on JBBQ
by instruction-tuning, their bias scores become larger. In addition, augmenting
their prompts with warning about social biases reduces the effect of biases in
some models.
</summary>
    <author>
      <name>Hitomi Yanaka</name>
    </author>
    <author>
      <name>Namgi Han</name>
    </author>
    <author>
      <name>Ryoma Kumon</name>
    </author>
    <author>
      <name>Jie Lu</name>
    </author>
    <author>
      <name>Masashi Takeshita</name>
    </author>
    <author>
      <name>Ryo Sekizawa</name>
    </author>
    <author>
      <name>Taisei Kato</name>
    </author>
    <author>
      <name>Hiromi Arai</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02050v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02050v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02079v1</id>
    <updated>2024-06-04T08:00:40Z</updated>
    <published>2024-06-04T08:00:40Z</published>
    <title>Assessing the Performance of Chinese Open Source Large Language Models
  in Information Extraction Tasks</title>
    <summary>  Information Extraction (IE) plays a crucial role in Natural Language
Processing (NLP) by extracting structured information from unstructured text,
thereby facilitating seamless integration with various real-world applications
that rely on structured data. Despite its significance, recent experiments
focusing on English IE tasks have shed light on the challenges faced by Large
Language Models (LLMs) in achieving optimal performance, particularly in
sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a
comprehensive investigation of the performance of mainstream Chinese
open-source LLMs in tackling IE tasks, specifically under zero-shot conditions
where the models are not fine-tuned for specific tasks. Additionally, we
present the outcomes of several few-shot experiments to further gauge the
capability of these models. Moreover, our study includes a comparative analysis
between these open-source LLMs and ChatGPT, a widely recognized language model,
on IE performance. Through meticulous experimentation and analysis, we aim to
provide insights into the strengths, limitations, and potential enhancements of
existing Chinese open-source LLMs in the domain of Information Extraction
within the context of NLP.
</summary>
    <author>
      <name>Yida Cai</name>
    </author>
    <author>
      <name>Hao Sun</name>
    </author>
    <author>
      <name>Hsiu-Yuan Huang</name>
    </author>
    <author>
      <name>Yunfang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02100v1</id>
    <updated>2024-06-04T08:30:37Z</updated>
    <published>2024-06-04T08:30:37Z</published>
    <title>Exploring Mathematical Extrapolation of Large Language Models with
  Synthetic Data</title>
    <summary>  Large Language Models (LLMs) have shown excellent performance in language
understanding, text generation, code synthesis, and many other tasks, while
they still struggle in complex multi-step reasoning problems, such as
mathematical reasoning. In this paper, through a newly proposed arithmetical
puzzle problem, we show that the model can perform well on multi-step reasoning
tasks via fine-tuning on high-quality synthetic data. Experimental results with
the open-llama-3B model on three different test datasets show that not only the
model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also
demonstrates certain generalization capabilities on the out-of-domain datasets.
Specifically, this paper has designed two out-of-domain datasets in the form of
extending the numerical range and the composing components of the arithmetical
puzzle problem separately. The fine-tuned models have shown encouraging
performance on these two far more difficult tasks with the zero-shot pass@1 at
0.33 and 0.35, respectively.
</summary>
    <author>
      <name>Haolong Li</name>
    </author>
    <author>
      <name>Yu Ma</name>
    </author>
    <author>
      <name>Yinqi Zhang</name>
    </author>
    <author>
      <name>Chen Ye</name>
    </author>
    <author>
      <name>Jie Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accept by Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02106v1</id>
    <updated>2024-06-04T08:35:04Z</updated>
    <published>2024-06-04T08:35:04Z</published>
    <title>MARS: Benchmarking the Metaphysical Reasoning Abilities of Language
  Models with a Multi-task Evaluation Dataset</title>
    <summary>  To enable Large Language Models (LLMs) to function as conscious agents with
generalizable reasoning capabilities, it is crucial that they possess the
reasoning ability to comprehend situational changes (transitions) in
distribution triggered by environmental factors or actions from other agents.
Despite its fundamental significance, this ability remains underexplored due to
the complexity of modeling infinite possible changes in an event and their
associated distributions, coupled with the lack of benchmark data with
situational transitions. Addressing these gaps, we propose a novel formulation
of reasoning with distributional changes as a three-step discriminative
process, termed as MetAphysical ReaSoning. We then introduce the first-ever
benchmark, MARS, comprising three tasks corresponding to each step. These tasks
systematically assess LLMs' capabilities in reasoning the plausibility of (i)
changes in actions, (ii) states caused by changed actions, and (iii)
situational transitions driven by changes in action. Extensive evaluations with
20 (L)LMs of varying sizes and methods indicate that all three tasks in this
process pose significant challenges, even for state-of-the-art LLMs and LMs
after fine-tuning. Further analyses reveal potential causes for the
underperformance of LLMs and demonstrate that pre-training them on large-scale
conceptualization taxonomies can potentially enhance their metaphysical
reasoning capabilities. Our data and models are publicly accessible at
https://github.com/HKUST-KnowComp/MARS.
</summary>
    <author>
      <name>Weiqi Wang</name>
    </author>
    <author>
      <name>Yangqiu Song</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02120v1</id>
    <updated>2024-06-04T09:02:22Z</updated>
    <published>2024-06-04T09:02:22Z</published>
    <title>Diver: Large Language Model Decoding with Span-Level Mutual Information
  Verification</title>
    <summary>  Large language models (LLMs) have shown impressive capabilities in adapting
to various tasks when provided with task-specific instructions. However, LLMs
using standard decoding strategies often struggle with deviations from the
inputs. Intuitively, compliant LLM outputs should reflect the information
present in the input, which can be measured by point-wise mutual information
(PMI) scores. Therefore, we propose Diver, a novel approach that enhances LLM
Decoding through span-level PMI verification. During inference, Diver first
identifies divergence steps that may lead to multiple candidate spans.
Subsequently, it calculates the PMI scores by assessing the log-likelihood
gains of the input if the candidate spans are generated. Finally, the optimal
span is selected based on the PMI re-ranked output distributions. We evaluate
our method across various downstream tasks, and empirical results demonstrate
that Diver significantly outperforms existing decoding methods in both
performance and versatility.
</summary>
    <author>
      <name>Jinliang Lu</name>
    </author>
    <author>
      <name>Chen Wang</name>
    </author>
    <author>
      <name>Jiajun Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02134v1</id>
    <updated>2024-06-04T09:23:30Z</updated>
    <published>2024-06-04T09:23:30Z</published>
    <title>The current status of large language models in summarizing radiology
  report impressions</title>
    <summary>  Large language models (LLMs) like ChatGPT show excellent capabilities in
various natural language processing tasks, especially for text generation. The
effectiveness of LLMs in summarizing radiology report impressions remains
unclear. In this study, we explore the capability of eight LLMs on the
radiology report impression summarization. Three types of radiology reports,
i.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University
Cancer Hospital and Institute. We use the report findings to construct the
zero-shot, one-shot, and three-shot prompts with complete example reports to
generate the impressions. Besides the automatic quantitative evaluation
metrics, we define five human evaluation metrics, i.e., completeness,
correctness, conciseness, verisimilitude, and replaceability, to evaluate the
semantics of the generated impressions. Two thoracic surgeons (ZSY and LB) and
one radiologist (LQ) compare the generated impressions with the reference
impressions and score each impression under the five human evaluation metrics.
Experimental results show that there is a gap between the generated impressions
and reference impressions. Although the LLMs achieve comparable performance in
completeness and correctness, the conciseness and verisimilitude scores are not
very high. Using few-shot prompts can improve the LLMs' performance in
conciseness and verisimilitude, but the clinicians still think the LLMs can not
replace the radiologists in summarizing the radiology impressions.
</summary>
    <author>
      <name>Danqing Hu</name>
    </author>
    <author>
      <name>Shanyuan Zhang</name>
    </author>
    <author>
      <name>Qing Liu</name>
    </author>
    <author>
      <name>Xiaofeng Zhu</name>
    </author>
    <author>
      <name>Bing Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02143v1</id>
    <updated>2024-06-04T09:31:18Z</updated>
    <published>2024-06-04T09:31:18Z</published>
    <title>Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly
  with Large Language Models</title>
    <summary>  Learning multi-task models for jointly detecting stance and verifying rumors
poses challenges due to the need for training data of stance at post level and
rumor veracity at claim level, which are difficult to obtain. To address this
issue, we leverage large language models (LLMs) as the foundation annotators
for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed
as JSDRV. We introduce a novel reinforcement tuning framework to enhance the
joint predictive capabilities of LLM-based SD and RV components. Specifically,
we devise a policy for selecting LLM-annotated data at the two levels,
employing a hybrid reward mechanism to choose high-quality labels for effective
LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the
capabilities of LLMs in the joint tasks, not only outperforming
state-of-the-art methods but also generalizing to non-LLMs accommodated as task
models.
</summary>
    <author>
      <name>Ruichao Yang</name>
    </author>
    <author>
      <name>Wei Gao</name>
    </author>
    <author>
      <name>Jing Ma</name>
    </author>
    <author>
      <name>Hongzhan Lin</name>
    </author>
    <author>
      <name>Bo Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024 (Findings)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02169v2</id>
    <updated>2024-06-05T20:07:50Z</updated>
    <published>2024-06-04T09:58:29Z</published>
    <title>A multilingual dataset for offensive language and hate speech detection
  for hausa, yoruba and igbo languages</title>
    <summary>  The proliferation of online offensive language necessitates the development
of effective detection mechanisms, especially in multilingual contexts. This
study addresses the challenge by developing and introducing novel datasets for
offensive language detection in three major Nigerian languages: Hausa, Yoruba,
and Igbo. We collected data from Twitter and manually annotated it to create
datasets for each of the three languages, using native speakers. We used
pre-trained language models to evaluate their efficacy in detecting offensive
language in our datasets. The best-performing model achieved an accuracy of
90\%. To further support research in offensive language detection, we plan to
make the dataset and our models publicly available.
</summary>
    <author>
      <name>Saminu Mohammad Aliyu</name>
    </author>
    <author>
      <name>Gregory Maksha Wajiga</name>
    </author>
    <author>
      <name>Muhammad Murtala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The experimental result was erroneously reported and we also omitted
  other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02169v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02169v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14F05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02237v1</id>
    <updated>2024-06-04T11:57:58Z</updated>
    <published>2024-06-04T11:57:58Z</published>
    <title>Self-Modifying State Modeling for Simultaneous Machine Translation</title>
    <summary>  Simultaneous Machine Translation (SiMT) generates target outputs while
receiving stream source inputs and requires a read/write policy to decide
whether to wait for the next source token or generate a new target token, whose
decisions form a \textit{decision path}. Existing SiMT methods, which learn the
policy by exploring various decision paths in training, face inherent
limitations. These methods not only fail to precisely optimize the policy due
to the inability to accurately assess the individual impact of each decision on
SiMT performance, but also cannot sufficiently explore all potential paths
because of their vast number. Besides, building decision paths requires
unidirectional encoders to simulate streaming source inputs, which impairs the
translation quality of SiMT models. To solve these issues, we propose
\textbf{S}elf-\textbf{M}odifying \textbf{S}tate \textbf{M}odeling (SM$^2$), a
novel training paradigm for SiMT task. Without building decision paths, SM$^2$
individually optimizes decisions at each state during training. To precisely
optimize the policy, SM$^2$ introduces Self-Modifying process to independently
assess and adjust decisions at each state. For sufficient exploration, SM$^2$
proposes Prefix Sampling to efficiently traverse all potential states.
Moreover, SM$^2$ ensures compatibility with bidirectional encoders, thus
achieving higher translation quality. Experiments show that SM$^2$ outperforms
strong baselines. Furthermore, SM$^2$ allows offline machine translation models
to acquire SiMT ability with fine-tuning.
</summary>
    <author>
      <name>Donglei Yu</name>
    </author>
    <author>
      <name>Xiaomian Kang</name>
    </author>
    <author>
      <name>Yuchen Liu</name>
    </author>
    <author>
      <name>Yu Zhou</name>
    </author>
    <author>
      <name>Chengqing Zong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accept to ACL 2024 main conference. 15 pages, 13 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02266v1</id>
    <updated>2024-06-04T12:43:23Z</updated>
    <published>2024-06-04T12:43:23Z</published>
    <title>Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning
  Compressor</title>
    <summary>  Despite the prevalence of retrieval-augmented language models (RALMs), the
seamless integration of these models with retrieval mechanisms to enhance
performance in document-based tasks remains challenging. While some
post-retrieval processing Retrieval-Augmented Generation (RAG) methods have
achieved success, most still lack the ability to distinguish pertinent from
extraneous information, leading to potential inconsistencies and reduced
precision in the generated output, which subsequently affects the truthfulness
of the language model's responses. To address these limitations, this work
proposes a novel two-stage consistency learning approach for retrieved
information compression in retrieval-augmented language models to enhance
performance. By incorporating consistency learning, the aim is to generate
summaries that maintain coherence and alignment with the intended semantic
representations of a teacher model while improving faithfulness to the original
retrieved documents. The proposed method is empirically validated across
multiple datasets, demonstrating notable enhancements in precision and
efficiency for question-answering tasks. It outperforms existing baselines and
showcases the synergistic effects of combining contrastive and consistency
learning paradigms within the retrieval-augmented generation framework.
</summary>
    <author>
      <name>Chuankai Xu</name>
    </author>
    <author>
      <name>Dongming Zhao</name>
    </author>
    <author>
      <name>Bo Wang</name>
    </author>
    <author>
      <name>Hanwen Xing</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02267v1</id>
    <updated>2024-06-04T12:43:47Z</updated>
    <published>2024-06-04T12:43:47Z</published>
    <title>Prompting Large Language Models with Human Error Markings for
  Self-Correcting Machine Translation</title>
    <summary>  While large language models (LLMs) pre-trained on massive amounts of unpaired
language data have reached the state-of-the-art in machine translation (MT) of
general domain texts, post-editing (PE) is still required to correct errors and
to enhance term translation quality in specialized domains. In this paper we
present a pilot study of enhancing translation memories (TM) produced by PE
(source segments, machine translations, and reference translations, henceforth
called PE-TM) for the needs of correct and consistent term translation in
technical domains.
  We investigate a light-weight two-step scenario where, at inference time, a
human translator marks errors in the first translation step, and in a second
step a few similar examples are extracted from the PE-TM to prompt an LLM. Our
experiment shows that the additional effort of augmenting translations with
human error markings guides the LLM to focus on a correction of the marked
errors, yielding consistent improvements over automatic PE (APE) and MT from
scratch.
</summary>
    <author>
      <name>Nathaniel Berger</name>
    </author>
    <author>
      <name>Stefan Riezler</name>
    </author>
    <author>
      <name>Miriam Exel</name>
    </author>
    <author>
      <name>Matthias Huck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at The 25th Annual Conference of the European Association
  for Machine Translation (EAMT 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02301v1</id>
    <updated>2024-06-04T13:30:45Z</updated>
    <published>2024-06-04T13:30:45Z</published>
    <title>mCoT: Multilingual Instruction Tuning for Reasoning Consistency in
  Language Models</title>
    <summary>  Large language models (LLMs) with Chain-of-thought (CoT) have recently
emerged as a powerful technique for eliciting reasoning to improve various
downstream tasks. As most research mainly focuses on English, with few
explorations in a multilingual context, the question of how reliable this
reasoning capability is in different languages is still open. To address it
directly, we study multilingual reasoning consistency across multiple
languages, using popular open-source LLMs. First, we compile the first
large-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven
diverse languages. Then, we introduce multilingual CoT instruction tuning to
boost reasoning capability across languages, thereby improving model
consistency. While existing LLMs show substantial variation across the
languages we consider, and especially low performance for lesser resourced
languages, our 7B parameter model mCoT achieves impressive consistency across
languages, and superior or comparable performance to close- and open-source
models even of much larger sizes.
</summary>
    <author>
      <name>Huiyuan Lai</name>
    </author>
    <author>
      <name>Malvina Nissim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024 main</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02331v1</id>
    <updated>2024-06-04T14:00:02Z</updated>
    <published>2024-06-04T14:00:02Z</published>
    <title>Translation Deserves Better: Analyzing Translation Artifacts in
  Cross-lingual Visual Question Answering</title>
    <summary>  Building a reliable visual question answering~(VQA) system across different
languages is a challenging problem, primarily due to the lack of abundant
samples for training. To address this challenge, recent studies have employed
machine translation systems for the cross-lingual VQA task. This involves
translating the evaluation samples into a source language (usually English) and
using monolingual models (i.e., translate-test). However, our analysis reveals
that translated texts contain unique characteristics distinct from
human-written ones, referred to as translation artifacts. We find that these
artifacts can significantly affect the models, confirmed by extensive
experiments across diverse models, languages, and translation processes. In
light of this, we present a simple data augmentation strategy that can
alleviate the adverse impacts of translation artifacts.
</summary>
    <author>
      <name>ChaeHun Park</name>
    </author>
    <author>
      <name>Koanho Lee</name>
    </author>
    <author>
      <name>Hyesu Lim</name>
    </author>
    <author>
      <name>Jaeseok Kim</name>
    </author>
    <author>
      <name>Junmo Park</name>
    </author>
    <author>
      <name>Yu-Jung Heo</name>
    </author>
    <author>
      <name>Du-Seong Chang</name>
    </author>
    <author>
      <name>Jaegul Choo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024 Findings Accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02335v1</id>
    <updated>2024-06-04T14:06:03Z</updated>
    <published>2024-06-04T14:06:03Z</published>
    <title>Probing the Category of Verbal Aspect in Transformer Language Models</title>
    <summary>  We investigate how pretrained language models (PLM) encode the grammatical
category of verbal aspect in Russian. Encoding of aspect in transformer LMs has
not been studied previously in any language. A particular challenge is posed by
"alternative contexts": where either the perfective or the imperfective aspect
is suitable grammatically and semantically. We perform probing using BERT and
RoBERTa on alternative and non-alternative contexts. First, we assess the
models' performance on aspect prediction, via behavioral probing. Next, we
examine the models' performance when their contextual representations are
substituted with counterfactual representations, via causal probing. These
counterfactuals alter the value of the "boundedness" feature--a semantic
feature, which characterizes the action in the context. Experiments show that
BERT and RoBERTa do encode aspect--mostly in their final layers. The
counterfactual interventions affect perfective and imperfective in opposite
ways, which is consistent with grammar: perfective is positively affected by
adding the meaning of boundedness, and vice versa. The practical implications
of our probing results are that fine-tuning only the last layers of BERT on
predicting aspect is faster and more effective than fine-tuning the whole
model. The model has high predictive uncertainty about aspect in alternative
contexts, which tend to lack explicit hints about the boundedness of the
described action.
</summary>
    <author>
      <name>Anisia Katinskaia</name>
    </author>
    <author>
      <name>Roman Yangarber</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02376v1</id>
    <updated>2024-06-04T14:53:24Z</updated>
    <published>2024-06-04T14:53:24Z</published>
    <title>Retaining Key Information under High Compression Ratios: Query-Guided
  Compressor for LLMs</title>
    <summary>  The growing popularity of Large Language Models has sparked interest in
context compression for Large Language Models (LLMs). However, the performance
of previous methods degrades dramatically as compression ratios increase,
sometimes even falling to the closed-book level. This decline can be attributed
to the loss of key information during the compression process. Our preliminary
study supports this hypothesis, emphasizing the significance of retaining key
information to maintain model performance under high compression ratios. As a
result, we introduce Query-Guided Compressor (QGC), which leverages queries to
guide the context compression process, effectively preserving key information
within the compressed context. Additionally, we employ a dynamic compression
strategy. We validate the effectiveness of our proposed QGC on the Question
Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets.
Experimental results show that QGC can consistently perform well even at high
compression ratios, which also offers significant benefits in terms of
inference cost and throughput.
</summary>
    <author>
      <name>Zhiwei Cao</name>
    </author>
    <author>
      <name>Qian Cao</name>
    </author>
    <author>
      <name>Yu Lu</name>
    </author>
    <author>
      <name>Ningxin Peng</name>
    </author>
    <author>
      <name>Luyang Huang</name>
    </author>
    <author>
      <name>Shanbo Cheng</name>
    </author>
    <author>
      <name>Jinsong Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02378v1</id>
    <updated>2024-06-04T14:55:43Z</updated>
    <published>2024-06-04T14:55:43Z</published>
    <title>On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and
  Latent Concept</title>
    <summary>  Large Language Models (LLMs) can improve their responses when instructed to
do so, a capability known as self-correction. When these instructions lack
specific details about the issues in the response, this is referred to as
leveraging the intrinsic self-correction capability. The empirical success of
self-correction can be found in various applications, e.g., text detoxification
and social bias mitigation. However, leveraging this self-correction capability
may not always be effective, as it has the potential to revise an initially
correct response into an incorrect one. In this paper, we endeavor to
understand how and why leveraging the self-correction capability is effective.
We identify that appropriate instructions can guide LLMs to a convergence
state, wherein additional self-correction steps do not yield further
performance improvements. We empirically demonstrate that model uncertainty and
activated latent concepts jointly characterize the effectiveness of
self-correction. Furthermore, we provide a mathematical formulation indicating
that the activated latent concept drives the convergence of the model
uncertainty and self-correction performance. Our analysis can also be
generalized to the self-correction behaviors observed in Vision-Language Models
(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from
our principle in terms of selecting effective fine-tuning samples. Such initial
success demonstrates the potential extensibility for better instruction tuning
and safety alignment.
</summary>
    <author>
      <name>Guangliang Liu</name>
    </author>
    <author>
      <name>Haitao Mao</name>
    </author>
    <author>
      <name>Bochuan Cao</name>
    </author>
    <author>
      <name>Zhiyu Xue</name>
    </author>
    <author>
      <name>Kristen Johnson</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <author>
      <name>Rongrong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02472v1</id>
    <updated>2024-06-04T16:42:17Z</updated>
    <published>2024-06-04T16:42:17Z</published>
    <title>Analyzing Temporal Complex Events with Large Language Models? A
  Benchmark towards Temporal, Long Context Understanding</title>
    <summary>  The digital landscape is rapidly evolving with an ever-increasing volume of
online news, emphasizing the need for swift and precise analysis of complex
events. We refer to the complex events composed of many news articles over an
extended period as Temporal Complex Event (TCE). This paper proposes a novel
approach using Large Language Models (LLMs) to systematically extract and
analyze the event chain within TCE, characterized by their key points and
timestamps. We establish a benchmark, named TCELongBench, to evaluate the
proficiency of LLMs in handling temporal dynamics and understanding extensive
text. This benchmark encompasses three distinct tasks - reading comprehension,
temporal sequencing, and future event forecasting. In the experiment, we
leverage retrieval-augmented generation (RAG) method and LLMs with long context
window to deal with lengthy news articles of TCE. Our findings indicate that
models with suitable retrievers exhibit comparable performance with those
utilizing long context window.
</summary>
    <author>
      <name>Zhihan Zhang</name>
    </author>
    <author>
      <name>Yixin Cao</name>
    </author>
    <author>
      <name>Chenchen Ye</name>
    </author>
    <author>
      <name>Yunshan Ma</name>
    </author>
    <author>
      <name>Lizi Liao</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02517v1</id>
    <updated>2024-06-04T17:39:23Z</updated>
    <published>2024-06-04T17:39:23Z</published>
    <title>Deterministic Reversible Data Augmentation for Neural Machine
  Translation</title>
    <summary>  Data augmentation is an effective way to diversify corpora in machine
translation, but previous methods may introduce semantic inconsistency between
original and augmented data because of irreversible operations and random
subword sampling procedures. To generate both symbolically diverse and
semantically consistent augmentation data, we propose Deterministic Reversible
Data Augmentation (DRDA), a simple but effective data augmentation method for
neural machine translation. DRDA adopts deterministic segmentations and
reversible operations to generate multi-granularity subword representations and
pulls them closer together with multi-view techniques. With no extra corpora or
model changes required, DRDA outperforms strong baselines on several
translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer)
and exhibits good robustness in noisy, low-resource, and cross-domain datasets.
</summary>
    <author>
      <name>Jiashu Yao</name>
    </author>
    <author>
      <name>Heyan Huang</name>
    </author>
    <author>
      <name>Zeming Liu</name>
    </author>
    <author>
      <name>Yuhang Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02524v2</id>
    <updated>2024-06-07T17:58:22Z</updated>
    <published>2024-06-04T17:42:21Z</published>
    <title>CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks</title>
    <summary>  Large Language Models (LLMs) are revolutionizing various domains, yet
verifying their answers remains a significant challenge, especially for
intricate open-ended tasks such as consolidation, summarization, and extraction
of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and
simple LLM verification approach. CheckEmbed is driven by a straightforward yet
powerful idea: in order to compare LLM solutions to one another or to the
ground-truth, compare their corresponding answer-level embeddings obtained with
a model such as GPT Text Embedding Large. This reduces a complex textual answer
to a single embedding, facilitating straightforward, fast, and meaningful
verification. We develop a comprehensive verification pipeline implementing the
CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for
assessing the truthfulness of the LLM answers, such as embedding heatmaps and
their summaries. We show how to use these metrics for deploying practical
engines that decide whether an LLM answer is satisfactory or not. We apply the
pipeline to real-world document analysis tasks, including term extraction and
document summarization, showcasing significant improvements in accuracy,
cost-effectiveness, and runtime performance compared to existing token-,
sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.
</summary>
    <author>
      <name>Maciej Besta</name>
    </author>
    <author>
      <name>Lorenzo Paleari</name>
    </author>
    <author>
      <name>Ales Kubicek</name>
    </author>
    <author>
      <name>Piotr Nyczyk</name>
    </author>
    <author>
      <name>Robert Gerstenberger</name>
    </author>
    <author>
      <name>Patrick Iff</name>
    </author>
    <author>
      <name>Tomasz Lehmann</name>
    </author>
    <author>
      <name>Hubert Niewiadomski</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02528v2</id>
    <updated>2024-06-10T14:55:29Z</updated>
    <published>2024-06-04T17:50:34Z</published>
    <title>Scalable MatMul-free Language Modeling</title>
    <summary>  Matrix multiplication (MatMul) typically dominates the overall computational
cost of large language models (LLMs). This cost only grows as LLMs scale to
larger embedding dimensions and context lengths. In this work, we show that
MatMul operations can be completely eliminated from LLMs while maintaining
strong performance at billion-parameter scales. Our experiments show that our
proposed MatMul-free models achieve performance on-par with state-of-the-art
Transformers that require far more memory during inference at a scale up to at
least 2.7B parameters. We investigate the scaling laws and find that the
performance gap between our MatMul-free models and full precision Transformers
narrows as the model size increases. We also provide a GPU-efficient
implementation of this model which reduces memory usage by up to 61% over an
unoptimized baseline during training. By utilizing an optimized kernel during
inference, our model's memory consumption can be reduced by more than 10x
compared to unoptimized models. To properly quantify the efficiency of our
architecture, we build a custom hardware solution on an FPGA which exploits
lightweight operations beyond what GPUs are capable of. We processed
billion-parameter scale models at 13W beyond human readable throughput, moving
LLMs closer to brain-like efficiency. This work not only shows how far LLMs can
be stripped back while still performing effectively, but also points at the
types of operations future accelerators should be optimized for in processing
the next generation of lightweight LLMs. Our code implementation is available
at https://github.com/ridgerchu/matmulfreellm.
</summary>
    <author>
      <name>Rui-Jie Zhu</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Ethan Sifferman</name>
    </author>
    <author>
      <name>Tyler Sheaves</name>
    </author>
    <author>
      <name>Yiqiao Wang</name>
    </author>
    <author>
      <name>Dustin Richmond</name>
    </author>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Jason K. Eshraghian</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02528v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02528v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02532v1</id>
    <updated>2024-06-04T17:53:36Z</updated>
    <published>2024-06-04T17:53:36Z</published>
    <title>SpecExec: Massively Parallel Speculative Decoding for Interactive LLM
  Inference on Consumer Devices</title>
    <summary>  As large language models gain widespread adoption, running them efficiently
becomes crucial. Recent works on LLM inference use speculative decoding to
achieve extreme speedups. However, most of these works implicitly design their
algorithms for high-end datacenter hardware. In this work, we ask the opposite
question: how fast can we run LLMs on consumer machines? Consumer GPUs can no
longer fit the largest available models (50B+ parameters) and must offload them
to RAM or SSD. When running with offloaded parameters, the inference engine can
process batches of hundreds or thousands of tokens at the same time as just one
token, making it a natural fit for speculative decoding. We propose SpecExec
(Speculative Execution), a simple parallel decoding method that can generate up
to 20 tokens per target model iteration for popular LLM families. It utilizes
the high spikiness of the token probabilities distribution in modern LLMs and a
high degree of alignment between model output probabilities. SpecExec takes the
most probable tokens continuation from the draft model to build a "cache" tree
for the target model, which then gets validated in a single pass. Using
SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with
RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens
per second with 16-bit weights.
</summary>
    <author>
      <name>Ruslan Svirschevski</name>
    </author>
    <author>
      <name>Avner May</name>
    </author>
    <author>
      <name>Zhuoming Chen</name>
    </author>
    <author>
      <name>Beidi Chen</name>
    </author>
    <author>
      <name>Zhihao Jia</name>
    </author>
    <author>
      <name>Max Ryabinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint. arXiv admin note: text overlap with arXiv:2312.17238 by
  other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02746v2</id>
    <updated>2024-06-09T05:37:26Z</updated>
    <published>2024-06-04T20:02:52Z</published>
    <title>RATT: A Thought Structure for Coherent and Correct LLM Reasoning</title>
    <summary>  Large Language Models (LLMs) gain substantial reasoning and decision-making
capabilities from thought structures. However, existing methods such as Tree of
Thought and Retrieval Augmented Thoughts often fall short in complex tasks due
to the limitations of insufficient local retrieval of factual knowledge and
inadequate global selection of strategies. These limitations make it
challenging for these methods to balance factual accuracy and comprehensive
logical optimization effectively. To address these limitations, we introduce
the Retrieval Augmented Thought Tree (RATT), a novel thought structure that
considers both overall logical soundness and factual correctness at each step
of the thinking process. Specifically, at every point of a thought branch, RATT
performs planning and lookahead to explore and evaluate multiple potential
reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented
Generation (RAG) with LLM's ability to assess overall strategy. Through this
combination of factual knowledge and strategic feasibility, the RATT adjusts
and integrates the thought tree structure to search for the most promising
branches within the search space. This thought structure significantly enhances
the model's coherence in logical inference and efficiency in decision-making,
and thus increases the limit of the capacity of LLM to generate reliable
inferences and decisions based on thought structures. A broad range of
experiments on different types of tasks showcases that the RATT structure
significantly outperforms existing methods in factual correctness and logical
coherence.
</summary>
    <author>
      <name>Jinghan Zhang</name>
    </author>
    <author>
      <name>Xiting Wang</name>
    </author>
    <author>
      <name>Weijieying Ren</name>
    </author>
    <author>
      <name>Lu Jiang</name>
    </author>
    <author>
      <name>Dongjie Wang</name>
    </author>
    <author>
      <name>Kunpeng Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02818v1</id>
    <updated>2024-06-04T23:36:08Z</updated>
    <published>2024-06-04T23:36:08Z</published>
    <title>Chain of Agents: Large Language Models Collaborating on Long-Context
  Tasks</title>
    <summary>  Addressing the challenge of effectively processing long contexts has become a
critical issue for Large Language Models (LLMs). Two common strategies have
emerged: 1) reducing the input length, such as retrieving relevant chunks by
Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit
of LLMs. However, both strategies have drawbacks: input reduction has no
guarantee of covering the part with needed information, while window extension
struggles with focusing on the pertinent information for solving the task. To
mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework
that harnesses multi-agent collaboration through natural language to enable
information aggregation and context reasoning across various LLMs over
long-context tasks. CoA consists of multiple worker agents who sequentially
communicate to handle different segmented portions of the text, followed by a
manager agent who synthesizes these contributions into a coherent final output.
CoA processes the entire input by interleaving reading and reasoning, and it
mitigates long context focus issues by assigning each agent a short context. We
perform comprehensive evaluation of CoA on a wide range of long-context tasks
in question answering, summarization, and code completion, demonstrating
significant improvements by up to 10% over strong baselines of RAG,
Full-Context, and multi-agent LLMs.
</summary>
    <author>
      <name>Yusen Zhang</name>
    </author>
    <author>
      <name>Ruoxi Sun</name>
    </author>
    <author>
      <name>Yanfei Chen</name>
    </author>
    <author>
      <name>Tomas Pfister</name>
    </author>
    <author>
      <name>Rui Zhang</name>
    </author>
    <author>
      <name>Sercan Ö. Arik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02830v1</id>
    <updated>2024-06-05T00:31:50Z</updated>
    <published>2024-06-05T00:31:50Z</published>
    <title>Too Big to Fail: Larger Language Models are Disproportionately Resilient
  to Induction of Dementia-Related Linguistic Anomalies</title>
    <summary>  As artificial neural networks grow in complexity, understanding their inner
workings becomes increasingly challenging, which is particularly important in
healthcare applications. The intrinsic evaluation metrics of autoregressive
neural language models (NLMs), perplexity (PPL), can reflect how "surprised" an
NLM model is at novel input. PPL has been widely used to understand the
behavior of NLMs. Previous findings show that changes in PPL when masking
attention layers in pre-trained transformer-based NLMs reflect linguistic
anomalies associated with Alzheimer's disease dementia. Building upon this, we
explore a novel bidirectional attention head ablation method that exhibits
properties attributed to the concepts of cognitive and brain reserve in human
brain studies, which postulate that people with more neurons in the brain and
more efficient processing are more resilient to neurodegeneration. Our results
show that larger GPT-2 models require a disproportionately larger share of
attention heads to be masked/ablated to display degradation of similar
magnitude to masking in smaller models. These results suggest that the
attention mechanism in transformer models may present an analogue to the
notions of cognitive and brain reserve and could potentially be used to model
certain aspects of the progression of neurodegenerative disorders and aging.
</summary>
    <author>
      <name>Changye Li</name>
    </author>
    <author>
      <name>Zhecheng Sheng</name>
    </author>
    <author>
      <name>Trevor Cohen</name>
    </author>
    <author>
      <name>Serguei Pakhomov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024 findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02863v1</id>
    <updated>2024-06-05T02:25:10Z</updated>
    <published>2024-06-05T02:25:10Z</published>
    <title>LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation</title>
    <summary>  This research investigates the effect of prompt design on dialogue evaluation
using large language models (LLMs). While LLMs are increasingly used for
scoring various inputs, creating effective prompts for dialogue evaluation
remains challenging due to model sensitivity and subjectivity in dialogue
assessments. Our study experimented with different prompt structures, altering
the sequence of output instructions and including explanatory reasons. We found
that the order of presenting reasons and scores significantly influences LLMs'
scoring, with a "reason-first" approach yielding more comprehensive
evaluations. This insight is crucial for enhancing the accuracy and consistency
of LLM-based evaluations.
</summary>
    <author>
      <name>Yi-Pei Chen</name>
    </author>
    <author>
      <name>KuanChao Chu</name>
    </author>
    <author>
      <name>Hideki Nakayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in AAAI 2024 Spring Symposium. The first two authors
  contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02893v2</id>
    <updated>2024-06-09T10:53:29Z</updated>
    <published>2024-06-05T03:26:59Z</published>
    <title>Language Model Can Do Knowledge Tracing: Simple but Effective Method to
  Integrate Language Model and Knowledge Tracing Task</title>
    <summary>  Knowledge Tracing (KT) is a critical task in online learning for modeling
student knowledge over time. Despite the success of deep learning-based KT
models, which rely on sequences of numbers as data, most existing approaches
fail to leverage the rich semantic information in the text of questions and
concepts. This paper proposes Language model-based Knowledge Tracing (LKT), a
novel framework that integrates pre-trained language models (PLMs) with KT
methods. By leveraging the power of language models to capture semantic
representations, LKT effectively incorporates textual information and
significantly outperforms previous KT models on large benchmark datasets.
Moreover, we demonstrate that LKT can effectively address the cold-start
problem in KT by leveraging the semantic knowledge captured by PLMs.
Interpretability of LKT is enhanced compared to traditional KT models due to
its use of text-rich data. We conducted the local interpretable model-agnostic
explanation technique and analysis of attention scores to interpret the model
performance further. Our work highlights the potential of integrating PLMs with
KT and paves the way for future research in KT domain.
</summary>
    <author>
      <name>Unggi Lee</name>
    </author>
    <author>
      <name>Jiyeong Bae</name>
    </author>
    <author>
      <name>Dohee Kim</name>
    </author>
    <author>
      <name>Sookbun Lee</name>
    </author>
    <author>
      <name>Jaekwon Park</name>
    </author>
    <author>
      <name>Taekyung Ahn</name>
    </author>
    <author>
      <name>Gunho Lee</name>
    </author>
    <author>
      <name>Damji Stratton</name>
    </author>
    <author>
      <name>Hyeoncheol Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02902v2</id>
    <updated>2024-06-07T07:32:50Z</updated>
    <published>2024-06-05T03:44:35Z</published>
    <title>S$^2$GSL: Incorporating Segment to Syntactic Enhanced Graph Structure
  Learning for Aspect-based Sentiment Analysis</title>
    <summary>  Previous graph-based approaches in Aspect based Sentiment Analysis(ABSA) have
demonstrated impressive performance by utilizing graph neural networks and
attention mechanisms to learn structures of static dependency trees and dynamic
latent trees. However, incorporating both semantic and syntactic information
simultaneously within complex global structures can introduce irrelevant
contexts and syntactic dependencies during the process of graph structure
learning, potentially resulting in inaccurate predictions. In order to address
the issues above, we propose S$^2$GSL, incorporating Segment to Syntactic
enhanced Graph Structure Learning for ABSA. Specifically,S$^2$GSL is featured
with a segment-aware semantic graph learning and a syntax-based latent graph
learning enabling the removal of irrelevant contexts and dependencies,
respectively. We further propose a self-adaptive aggregation network that
facilitates the fusion of two graph learning branches, thereby achieving
complementarity across diverse structures. Experimental results on four
benchmarks demonstrate the effectiveness of our framework.
</summary>
    <author>
      <name>Bingfeng Chen</name>
    </author>
    <author>
      <name>Qihan Ouyang</name>
    </author>
    <author>
      <name>Yongqi Luo</name>
    </author>
    <author>
      <name>Boyan Xu</name>
    </author>
    <author>
      <name>Ruichu Cai</name>
    </author>
    <author>
      <name>Zhifeng Hao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL2024(main)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02903v1</id>
    <updated>2024-06-05T03:46:52Z</updated>
    <published>2024-06-05T03:46:52Z</published>
    <title>Open Grounded Planning: Challenges and Benchmark Construction</title>
    <summary>  The emergence of large language models (LLMs) has increasingly drawn
attention to the use of LLMs for human-like planning. Existing work on
LLM-based planning either focuses on leveraging the inherent language
generation capabilities of LLMs to produce free-style plans, or employs
reinforcement learning approaches to learn decision-making for a limited set of
actions within restricted environments. However, both approaches exhibit
significant discrepancies from the open and executable requirements in
real-world planning. In this paper, we propose a new planning task--open
grounded planning. The primary objective of open grounded planning is to ask
the model to generate an executable plan based on a variable action set,
thereby ensuring the executability of the produced plan. To this end, we
establishes a benchmark for open grounded planning spanning a wide range of
domains. Then we test current state-of-the-art LLMs along with five planning
approaches, revealing that existing LLMs and methods still struggle to address
the challenges posed by grounded planning in open domains. The outcomes of this
paper define and establish a foundational dataset for open grounded planning,
and shed light on the potential challenges and future directions of LLM-based
planning.
</summary>
    <author>
      <name>Shiguang Guo</name>
    </author>
    <author>
      <name>Ziliang Deng</name>
    </author>
    <author>
      <name>Hongyu Lin</name>
    </author>
    <author>
      <name>Yaojie Lu</name>
    </author>
    <author>
      <name>Xianpei Han</name>
    </author>
    <author>
      <name>Le Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accept to ACL 2024 main conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02911v1</id>
    <updated>2024-06-05T04:04:08Z</updated>
    <published>2024-06-05T04:04:08Z</published>
    <title>Improving In-Context Learning with Prediction Feedback for Sentiment
  Analysis</title>
    <summary>  Large language models (LLMs) have achieved promising results in sentiment
analysis through the in-context learning (ICL) paradigm. However, their ability
to distinguish subtle sentiments still remains a challenge. Inspired by the
human ability to adjust understanding via feedback, this paper enhances ICL by
incorporating prior predictions and feedback, aiming to rectify sentiment
misinterpretation of LLMs. Specifically, the proposed framework consists of
three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive
feedback based on correctness, and (3) leveraging a feedback-driven prompt to
refine sentiment understanding. Experimental results across nine sentiment
analysis datasets demonstrate the superiority of our framework over
conventional ICL methods, with an average F1 improvement of 5.95%.
</summary>
    <author>
      <name>Hongling Xu</name>
    </author>
    <author>
      <name>Qianlong Wang</name>
    </author>
    <author>
      <name>Yice Zhang</name>
    </author>
    <author>
      <name>Min Yang</name>
    </author>
    <author>
      <name>Xi Zeng</name>
    </author>
    <author>
      <name>Bing Qin</name>
    </author>
    <author>
      <name>Ruifeng Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2024 (Findings)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02919v1</id>
    <updated>2024-06-05T04:15:07Z</updated>
    <published>2024-06-05T04:15:07Z</published>
    <title>MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering
  Medical Knowledge</title>
    <summary>  Large language models (LLMs) have excelled across domains, also delivering
notable performance on the medical evaluation benchmarks, such as MedQA.
However, there still exists a significant gap between the reported performance
and the practical effectiveness in real-world medical scenarios. In this paper,
we aim to explore the causes of this gap by employing a multifaceted
examination schema to systematically probe the actual mastery of medical
knowledge by current LLMs. Specifically, we develop a novel evaluation
framework MultifacetEval to examine the degree and coverage of LLMs in encoding
and mastering medical knowledge at multiple facets (comparison, rectification,
discrimination, and verification) concurrently. Based on the MultifacetEval
framework, we construct two multifaceted evaluation datasets: MultiDiseK (by
producing questions from a clinical disease knowledge base) and MultiMedQA (by
rephrasing each question from a medical benchmark MedQA into multifaceted
questions). The experimental results on these multifaceted datasets demonstrate
that the extent of current LLMs in mastering medical knowledge is far below
their performance on existing medical benchmarks, suggesting that they lack
depth, precision, and comprehensiveness in mastering medical knowledge.
Consequently, current LLMs are not yet ready for application in real-world
medical tasks. The codes and datasets are available at
https://github.com/THUMLP/MultifacetEval.
</summary>
    <author>
      <name>Yuxuan Zhou</name>
    </author>
    <author>
      <name>Xien Liu</name>
    </author>
    <author>
      <name>Chen Ning</name>
    </author>
    <author>
      <name>Ji Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.02974v1</id>
    <updated>2024-06-05T06:15:48Z</updated>
    <published>2024-06-05T06:15:48Z</published>
    <title>Readability-guided Idiom-aware Sentence Simplification (RISS) for
  Chinese</title>
    <summary>  Chinese sentence simplification faces challenges due to the lack of
large-scale labeled parallel corpora and the prevalence of idioms. To address
these challenges, we propose Readability-guided Idiom-aware Sentence
Simplification (RISS), a novel framework that combines data augmentation
techniques with lexcial simplification. RISS introduces two key components: (1)
Readability-guided Paraphrase Selection (RPS), a method for mining high-quality
sentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances
the comprehension and simplification of idiomatic expressions. By integrating
RPS and IAS using multi-stage and multi-task learning strategies, RISS
outperforms previous state-of-the-art methods on two Chinese sentence
simplification datasets. Furthermore, RISS achieves additional improvements
when fine-tuned on a small labeled dataset. Our approach demonstrates the
potential for more effective and accessible Chinese text simplification.
</summary>
    <author>
      <name>Jingshen Zhang</name>
    </author>
    <author>
      <name>Xinglu Chen</name>
    </author>
    <author>
      <name>Xinying Qiu</name>
    </author>
    <author>
      <name>Zhimin Wang</name>
    </author>
    <author>
      <name>Wenhe Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 23rd China National Conference on Computational
  Linguistics (CCL 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.02974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03062v1</id>
    <updated>2024-06-05T08:43:11Z</updated>
    <published>2024-06-05T08:43:11Z</published>
    <title>RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence
  Models for Abstractive Radiology Report Summarization</title>
    <summary>  Radiology report summarization is a crucial task that can help doctors
quickly identify clinically significant findings without the need to review
detailed sections of reports. This study proposes RadBARTsum, a domain-specific
and ontology facilitated adaptation of the BART model for abstractive radiology
report summarization. The approach involves two main steps: 1) re-training the
BART model on a large corpus of radiology reports using a novel entity masking
strategy to improving biomedical domain knowledge learning, and 2) fine-tuning
the model for the summarization task using the Findings and Background sections
to predict the Impression section. Experiments are conducted using different
masking strategies. Results show that the re-training process with domain
knowledge facilitated masking improves performances consistently across various
settings. This work contributes a domain-specific generative language model for
radiology report summarization and a method for utilising medical knowledge to
realise entity masking language model. The proposed approach demonstrates a
promising direction of enhancing the efficiency of language models by deepening
its understanding of clinical knowledge in radiology reports.
</summary>
    <author>
      <name>Jinge Wu</name>
    </author>
    <author>
      <name>Abul Hasan</name>
    </author>
    <author>
      <name>Honghan Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03075v1</id>
    <updated>2024-06-05T08:59:45Z</updated>
    <published>2024-06-05T08:59:45Z</published>
    <title>Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent
  Debate Framework</title>
    <summary>  The advent of large language models (LLMs) has facilitated the development of
natural language text generation. It also poses unprecedented challenges, with
content hallucination emerging as a significant concern. Existing solutions
often involve expensive and complex interventions during the training process.
Moreover, some approaches emphasize problem disassembly while neglecting the
crucial validation process, leading to performance degradation or limited
applications. To overcome these limitations, we propose a Markov Chain-based
multi-agent debate verification framework to enhance hallucination detection
accuracy in concise claims. Our method integrates the fact-checking process,
including claim detection, evidence retrieval, and multi-agent verification. In
the verification stage, we deploy multiple agents through flexible Markov
Chain-based debates to validate individual claims, ensuring meticulous
verification outcomes. Experimental results across three generative tasks
demonstrate that our approach achieves significant improvements over baselines.
</summary>
    <author>
      <name>Xiaoxi Sun</name>
    </author>
    <author>
      <name>Jinpeng Li</name>
    </author>
    <author>
      <name>Yan Zhong</name>
    </author>
    <author>
      <name>Dongyan Zhao</name>
    </author>
    <author>
      <name>Rui Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03092v1</id>
    <updated>2024-06-05T09:31:37Z</updated>
    <published>2024-06-05T09:31:37Z</published>
    <title>FragRel: Exploiting Fragment-level Relations in the External Memory of
  Large Language Models</title>
    <summary>  To process contexts with unlimited length using Large Language Models (LLMs),
recent studies explore hierarchically managing the long text. Only several text
fragments are taken from the external memory and passed into the temporary
working memory, i.e., LLM's context window. However, existing approaches
isolatedly handle the text fragments without considering their structural
connections, thereby suffering limited capability on texts with intensive
inter-relations, e.g., coherent stories and code repositories. This work
attempts to resolve this by exploiting the fragment-level relations in external
memory. First, we formulate the fragment-level relations and present several
instantiations for different text types. Next, we introduce a relation-aware
fragment assessment criteria upon previous independent fragment assessment.
Finally, we present the fragment-connected Hierarchical Memory based LLM. We
validate the benefits of involving these relations on long story understanding,
repository-level code generation, and long-term chatting.
</summary>
    <author>
      <name>Xihang Yue</name>
    </author>
    <author>
      <name>Linchao Zhu</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03125v1</id>
    <updated>2024-06-05T10:20:10Z</updated>
    <published>2024-06-05T10:20:10Z</published>
    <title>Space Decomposition for Sentence Embedding</title>
    <summary>  Determining sentence pair similarity is crucial for various NLP tasks. A
common technique to address this is typically evaluated on a continuous
semantic textual similarity scale from 0 to 5. However, based on a linguistic
observation in STS annotation guidelines, we found that the score in the range
[4,5] indicates an upper-range sample, while the rest are lower-range samples.
This necessitates a new approach to treating the upper-range and lower-range
classes separately. In this paper, we introduce a novel embedding space
decomposition method called MixSP utilizing a Mixture of Specialized
Projectors, designed to distinguish and rank upper-range and lower-range
samples accurately. The experimental results demonstrate that MixSP decreased
the overlap representation between upper-range and lower-range classes
significantly while outperforming competitors on STS and zero-shot benchmarks.
</summary>
    <author>
      <name>Wuttikorn Ponwitayarat</name>
    </author>
    <author>
      <name>Peerat Limkonchotiwat</name>
    </author>
    <author>
      <name>Ekapol Chuangsuwanich</name>
    </author>
    <author>
      <name>Sarana Nutanong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL Finding 2024. The code and pre-trained models are available at
  https://github.com/KornWtp/MixSP</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03127v1</id>
    <updated>2024-06-05T10:22:27Z</updated>
    <published>2024-06-05T10:22:27Z</published>
    <title>Towards Real-world Scenario: Imbalanced New Intent Discovery</title>
    <summary>  New Intent Discovery (NID) aims at detecting known and previously undefined
categories of user intent by utilizing limited labeled and massive unlabeled
data. Most prior works often operate under the unrealistic assumption that the
distribution of both familiar and new intent classes is uniform, overlooking
the skewed and long-tailed distributions frequently encountered in real-world
scenarios. To bridge the gap, our work introduces the imbalanced new intent
discovery (i-NID) task, which seeks to identify familiar and novel intent
categories within long-tailed distributions. A new benchmark (ImbaNID-Bench)
comprised of three datasets is created to simulate the real-world long-tail
distributions. ImbaNID-Bench ranges from broad cross-domain to specific
single-domain intent categories, providing a thorough representation of
practical use cases. Besides, a robust baseline model ImbaNID is proposed to
achieve cluster-friendly intent representations. It includes three stages:
model pre-training, generation of reliable pseudo-labels, and robust
representation learning that strengthens the model performance to handle the
intricacies of real-world data distributions. Our extensive experiments on
previous benchmarks and the newly established benchmark demonstrate the
superior performance of ImbaNID in addressing the i-NID task, highlighting its
potential as a powerful baseline for uncovering and categorizing user intents
in imbalanced and long-tailed
distributions\footnote{\url{https://github.com/Zkdc/i-NID}}.
</summary>
    <author>
      <name>Shun Zhang</name>
    </author>
    <author>
      <name>Chaoran Yan</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Jiaheng Liu</name>
    </author>
    <author>
      <name>Ying Mo</name>
    </author>
    <author>
      <name>Jiaqi Bai</name>
    </author>
    <author>
      <name>Tongliang Li</name>
    </author>
    <author>
      <name>Zhoujun Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03170v2</id>
    <updated>2024-06-06T08:29:23Z</updated>
    <published>2024-06-05T12:03:19Z</published>
    <title>StatBot.Swiss: Bilingual Open Data Exploration in Natural Language</title>
    <summary>  The potential for improvements brought by Large Language Models (LLMs) in
Text-to-SQL systems is mostly assessed on monolingual English datasets.
However, LLMs' performance for other languages remains vastly unexplored. In
this work, we release the StatBot.Swiss dataset, the first bilingual benchmark
for evaluating Text-to-SQL systems based on real-world applications. The
StatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big
databases with varying level of complexity for both English and German.
  We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo
and mixtral-8x7b-instruct for the Text-to-SQL translation task using an
in-context learning approach. Our experimental analysis illustrates that
current LLMs struggle to generalize well in generating SQL queries on our novel
bilingual dataset.
</summary>
    <author>
      <name>Farhad Nooralahzadeh</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Ellery Smith</name>
    </author>
    <author>
      <name>Sabine Maennel</name>
    </author>
    <author>
      <name>Cyril Matthey-Doret</name>
    </author>
    <author>
      <name>Raphaël de Fondville</name>
    </author>
    <author>
      <name>Kurt Stockinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work is accepted at ACL Findings 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03181v1</id>
    <updated>2024-06-05T12:11:10Z</updated>
    <published>2024-06-05T12:11:10Z</published>
    <title>Missci: Reconstructing Fallacies in Misrepresented Science</title>
    <summary>  Health-related misinformation on social networks can lead to poor
decision-making and real-world dangers. Such misinformation often misrepresents
scientific publications and cites them as "proof" to gain perceived
credibility. To effectively counter such claims automatically, a system must
explain how the claim was falsely derived from the cited publication. Current
methods for automated fact-checking or fallacy detection neglect to assess the
(mis)used evidence in relation to misinformation claims, which is required to
detect the mismatch between them. To address this gap, we introduce Missci, a
novel argumentation theoretical model for fallacious reasoning together with a
new dataset for real-world misinformation detection that misrepresents
biomedical publications. Unlike previous fallacy detection datasets, Missci (i)
focuses on implicit fallacies between the relevant content of the cited
publication and the inaccurate claim, and (ii) requires models to verbalize the
fallacious reasoning in addition to classifying it. We present Missci as a
dataset to test the critical reasoning abilities of large language models
(LLMs), that are required to reconstruct real-world fallacious arguments, in a
zero-shot setting. We evaluate two representative LLMs and the impact of
different levels of detail about the fallacy classes provided to the LLM via
prompts. Our experiments and human evaluation show promising results for GPT 4,
while also demonstrating the difficulty of this task.
</summary>
    <author>
      <name>Max Glockner</name>
    </author>
    <author>
      <name>Yufang Hou</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024 (main)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03239v1</id>
    <updated>2024-06-05T13:16:46Z</updated>
    <published>2024-06-05T13:16:46Z</published>
    <title>Document-level Claim Extraction and Decontextualisation for
  Fact-Checking</title>
    <summary>  Selecting which claims to check is a time-consuming task for human
fact-checkers, especially from documents consisting of multiple sentences and
containing multiple claims. However, existing claim extraction approaches focus
more on identifying and extracting claims from individual sentences, e.g.,
identifying whether a sentence contains a claim or the exact boundaries of the
claim within a sentence. In this paper, we propose a method for document-level
claim extraction for fact-checking, which aims to extract check-worthy claims
from documents and decontextualise them so that they can be understood out of
context. Specifically, we first recast claim extraction as extractive
summarization in order to identify central sentences from documents, then
rewrite them to include necessary context from the originating document through
sentence decontextualisation. Evaluation with both automatic metrics and a
fact-checking professional shows that our method is able to extract
check-worthy claims from documents more accurately than previous work, while
also improving evidence retrieval.
</summary>
    <author>
      <name>Zhenyun Deng</name>
    </author>
    <author>
      <name>Michael Schlichtkrul</name>
    </author>
    <author>
      <name>Andreas Vlachos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03363v1</id>
    <updated>2024-06-05T15:18:08Z</updated>
    <published>2024-06-05T15:18:08Z</published>
    <title>LLM-based Rewriting of Inappropriate Argumentation using Reinforcement
  Learning from Machine Feedback</title>
    <summary>  Ensuring that online discussions are civil and productive is a major
challenge for social media platforms. Such platforms usually rely both on users
and on automated detection tools to flag inappropriate arguments of other
users, which moderators then review. However, this kind of post-hoc moderation
is expensive and time-consuming, and moderators are often overwhelmed by the
amount and severity of flagged content. Instead, a promising alternative is to
prevent negative behavior during content creation. This paper studies how
inappropriate language in arguments can be computationally mitigated. We
propose a reinforcement learning-based rewriting approach that balances content
preservation and appropriateness based on existing classifiers, prompting an
instruction-finetuned large language model (LLM) as our initial policy. Unlike
related style transfer tasks, rewriting inappropriate arguments allows deleting
and adding content permanently. It is therefore tackled on document level
rather than sentence level. We evaluate different weighting schemes for the
reward function in both absolute and relative human assessment studies.
Systematic experiments on non-parallel data provide evidence that our approach
can mitigate the inappropriateness of arguments while largely preserving their
content. It significantly outperforms competitive baselines, including few-shot
learning, prompting, and humans.
</summary>
    <author>
      <name>Timon Ziegenbein</name>
    </author>
    <author>
      <name>Gabriella Skitalinskaya</name>
    </author>
    <author>
      <name>Alireza Bayat Makou</name>
    </author>
    <author>
      <name>Henning Wachsmuth</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03397v1</id>
    <updated>2024-06-05T15:54:50Z</updated>
    <published>2024-06-05T15:54:50Z</published>
    <title>Automating Turkish Educational Quiz Generation Using Large Language
  Models</title>
    <summary>  Crafting quizzes from educational content is a pivotal activity that benefits
both teachers and students by reinforcing learning and evaluating
understanding. In this study, we introduce a novel approach to generate quizzes
from Turkish educational texts, marking a pioneering endeavor in educational
technology specifically tailored to the Turkish educational context. We present
a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive
collection of Turkish educational texts accompanied by multiple-choice and
short-answer quizzes. This research leverages the capabilities of Large
Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo,
Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz
questions and answers from the Turkish educational content. Our work delineates
the methodology for employing these LLMs in the context of Turkish educational
material, thereby opening new avenues for automated Turkish quiz generation.
The study not only demonstrates the efficacy of using such models for
generating coherent and relevant quiz content but also sets a precedent for
future research in the domain of automated educational content creation for
languages other than English. The Turkish-Quiz-Instruct dataset is introduced
as a valuable resource for researchers and practitioners aiming to explore the
boundaries of educational technology and language-specific applications of LLMs
in Turkish. By addressing the challenges of quiz generation in a non-English
context specifically Turkish, this study contributes significantly to the field
of Turkish educational technology, providing insights into the potential of
leveraging LLMs for educational purposes across diverse linguistic landscapes.
</summary>
    <author>
      <name>Kamyar Zeinalipour</name>
    </author>
    <author>
      <name>Yusuf Gökberk Keptiğ</name>
    </author>
    <author>
      <name>Marco Maggini</name>
    </author>
    <author>
      <name>Marco Gori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted Paper for ISPR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03452v2</id>
    <updated>2024-06-06T06:53:36Z</updated>
    <published>2024-06-05T16:52:21Z</published>
    <title>Using Synchronic Definitions and Semantic Relations to Classify Semantic
  Change Types</title>
    <summary>  There is abundant evidence of the fact that the way words change their
meaning can be classified in different types of change, highlighting the
relationship between the old and new meanings (among which generalization,
specialization and co-hyponymy transfer). In this paper, we present a way of
detecting these types of change by constructing a model that leverages
information both from synchronic lexical relations and definitions of word
meanings. Specifically, we use synset definitions and hierarchy information
from WordNet and test it on a digitized version of Blank's (1997) dataset of
semantic change types. Finally, we show how the sense relationships can improve
models for both approximation of human judgments of semantic relatedness as
well as binary Lexical Semantic Change Detection.
</summary>
    <author>
      <name>Pierluigi Cassotti</name>
    </author>
    <author>
      <name>Stefano De Pascale</name>
    </author>
    <author>
      <name>Nina Tahmasebi</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03452v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03452v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03479v1</id>
    <updated>2024-06-05T17:32:28Z</updated>
    <published>2024-06-05T17:32:28Z</published>
    <title>MODABS: Multi-Objective Learning for Dynamic Aspect-Based Summarization</title>
    <summary>  The rapid proliferation of online content necessitates effective
summarization methods, among which dynamic aspect-based summarization stands
out. Unlike its traditional counterpart, which assumes a fixed set of known
aspects, this approach adapts to the varied aspects of the input text. We
introduce a novel multi-objective learning framework employing a
Longformer-Encoder-Decoder for this task. The framework optimizes aspect number
prediction, minimizes disparity between generated and reference summaries for
each aspect, and maximizes dissimilarity across aspect-specific summaries.
Extensive experiments show our method significantly outperforms baselines on
three diverse datasets, largely due to the effective alignment of generated and
reference aspect counts without sacrificing single-aspect summarization
quality.
</summary>
    <author>
      <name>Xiaobo Guo</name>
    </author>
    <author>
      <name>Soroush Vosoughi</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03486v1</id>
    <updated>2024-06-05T17:49:24Z</updated>
    <published>2024-06-05T17:49:24Z</published>
    <title>BIPED: Pedagogically Informed Tutoring System for ESL Education</title>
    <summary>  Large Language Models (LLMs) have a great potential to serve as readily
available and cost-efficient Conversational Intelligent Tutoring Systems (CITS)
for teaching L2 learners of English. Existing CITS, however, are designed to
teach only simple concepts or lack the pedagogical depth necessary to address
diverse learning strategies. To develop a more pedagogically informed CITS
capable of teaching complex concepts, we construct a BIlingual
PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human
English tutoring interactions. Through post-hoc analysis of the tutoring
interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9
student acts), which we use to further annotate the collected dataset. Based on
a two-step framework of first predicting the appropriate tutor act then
generating the corresponding response, we implemented two CITS models using
GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the
implemented models not only replicate the style of human teachers but also
employ diverse and contextually appropriate pedagogical strategies.
</summary>
    <author>
      <name>Soonwoo Kwon</name>
    </author>
    <author>
      <name>Sojung Kim</name>
    </author>
    <author>
      <name>Minju Park</name>
    </author>
    <author>
      <name>Seunghyun Lee</name>
    </author>
    <author>
      <name>Kyuseok Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03589v1</id>
    <updated>2024-06-05T19:14:21Z</updated>
    <published>2024-06-05T19:14:21Z</published>
    <title>Ranking Manipulation for Conversational Search Engines</title>
    <summary>  Major search engine providers are rapidly incorporating Large Language Model
(LLM)-generated content in response to user queries. These conversational
search engines operate by loading retrieved website text into the LLM context
for summarization and interpretation. Recent research demonstrates that LLMs
are highly vulnerable to jailbreaking and prompt injection attacks, which
disrupt the safety and quality goals of LLMs using adversarial strings. This
work investigates the impact of prompt injections on the ranking order of
sources referenced by conversational search engines. To this end, we introduce
a focused dataset of real-world consumer product websites and formalize
conversational search ranking as an adversarial problem. Experimentally, we
analyze conversational search rankings in the absence of adversarial injections
and show that different LLMs vary significantly in prioritizing product name,
document content, and context position. We then present a tree-of-attacks-based
jailbreaking technique which reliably promotes low-ranked products.
Importantly, these attacks transfer effectively to state-of-the-art
conversational search engines such as perplexity.ai. Given the strong financial
incentive for website owners to boost their search ranking, we argue that our
problem formulation is of critical importance for future robustness work.
</summary>
    <author>
      <name>Samuel Pfrommer</name>
    </author>
    <author>
      <name>Yatong Bai</name>
    </author>
    <author>
      <name>Tanmay Gautam</name>
    </author>
    <author>
      <name>Somayeh Sojoudi</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03618v1</id>
    <updated>2024-06-05T20:32:56Z</updated>
    <published>2024-06-05T20:32:56Z</published>
    <title>TACT: Advancing Complex Aggregative Reasoning with Information
  Extraction Tools</title>
    <summary>  Large Language Models (LLMs) often do not perform well on queries that
require the aggregation of information across texts. To better evaluate this
setting and facilitate modeling efforts, we introduce TACT - Text And
Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and
computational abilities using complex instructions. TACT contains challenging
instructions that demand stitching information scattered across one or more
texts, and performing complex integration on this information to generate the
answer. We construct this dataset by leveraging an existing dataset of texts
and their associated tables. For each such tables, we formulate new queries,
and gather their respective answers. We demonstrate that all contemporary LLMs
perform poorly on this dataset, achieving an accuracy below 38\%. To pinpoint
the difficulties and thoroughly dissect the problem, we analyze model
performance across three components: table-generation, Pandas
command-generation, and execution. Unexpectedly, we discover that each
component presents substantial challenges for current LLMs. These insights lead
us to propose a focused modeling framework, which we refer to as IE as a tool.
Specifically, we propose to add "tools" for each of the above steps, and
implement each such tool with few-shot prompting. This approach shows an
improvement over existing prompting techniques, offering a promising direction
for enhancing model capabilities in these tasks.
</summary>
    <author>
      <name>Avi Caciularu</name>
    </author>
    <author>
      <name>Alon Jacovi</name>
    </author>
    <author>
      <name>Eyal Ben-David</name>
    </author>
    <author>
      <name>Sasha Goldshtein</name>
    </author>
    <author>
      <name>Tal Schuster</name>
    </author>
    <author>
      <name>Jonathan Herzig</name>
    </author>
    <author>
      <name>Gal Elidan</name>
    </author>
    <author>
      <name>Amir Globerson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website (https://tact-benchmark.github.io), Huggingface
  (https://huggingface.co/datasets/google/TACT)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03666v1</id>
    <updated>2024-06-06T00:51:28Z</updated>
    <published>2024-06-06T00:51:28Z</published>
    <title>What Makes Language Models Good-enough?</title>
    <summary>  Psycholinguistic research suggests that humans may build a representation of
linguistic input that is 'good-enough' for the task at hand. This study
examines what architectural features make language models learn human-like
good-enough language processing. We focus on the number of layers and
self-attention heads in Transformers. We create a good-enough language
processing (GELP) evaluation dataset (7,680 examples), which is designed to
test the effects of two plausibility types, eight construction types, and three
degrees of memory cost on language processing. To annotate GELP, we first
conduct a crowdsourcing experiment whose design follows prior psycholinguistic
studies. Our model evaluation against the annotated GELP then reveals that the
full model as well as models with fewer layers and/or self-attention heads
exhibit a good-enough performance. This result suggests that models with
shallower depth and fewer heads can learn good-enough language processing.
</summary>
    <author>
      <name>Daiki Asami</name>
    </author>
    <author>
      <name>Saku Sugawara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Findings of ACL2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03699v1</id>
    <updated>2024-06-06T02:43:21Z</updated>
    <published>2024-06-06T02:43:21Z</published>
    <title>M-QALM: A Benchmark to Assess Clinical Reading Comprehension and
  Knowledge Recall in Large Language Models via Question Answering</title>
    <summary>  There is vivid research on adapting Large Language Models (LLMs) to perform a
variety of tasks in high-stakes domains such as healthcare. Despite their
popularity, there is a lack of understanding of the extent and contributing
factors that allow LLMs to recall relevant knowledge and combine it with
presented information in the clinical and biomedical domain: a fundamental
pre-requisite for success on down-stream tasks. Addressing this gap, we use
Multiple Choice and Abstractive Question Answering to conduct a large-scale
empirical study on 22 datasets in three generalist and three specialist
biomedical sub-domains. Our multifaceted analysis of the performance of 15
LLMs, further broken down by sub-domain, source of knowledge and model
architecture, uncovers success factors such as instruction tuning that lead to
improved recall and comprehension. We further show that while recently proposed
domain-adapted models may lack adequate knowledge, directly fine-tuning on our
collected medical knowledge datasets shows encouraging results, even
generalising to unseen specialist sub-domains. We complement the quantitative
results with a skill-oriented manual error analysis, which reveals a
significant gap between the models' capabilities to simply recall necessary
knowledge and to integrate it with the presented context. To foster research
and collaboration in this field we share M-QALM, our resources, standardised
methodology, and evaluation results, with the research community to facilitate
further advancements in clinical knowledge representation learning within
language models.
</summary>
    <author>
      <name>Anand Subramanian</name>
    </author>
    <author>
      <name>Viktor Schlegel</name>
    </author>
    <author>
      <name>Abhinav Ramesh Kashyap</name>
    </author>
    <author>
      <name>Thanh-Tung Nguyen</name>
    </author>
    <author>
      <name>Vijay Prakash Dwivedi</name>
    </author>
    <author>
      <name>Stefan Winkler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACL 2024 (Findings)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03725v1</id>
    <updated>2024-06-06T03:46:59Z</updated>
    <published>2024-06-06T03:46:59Z</published>
    <title>LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text
  Classification</title>
    <summary>  With the booming of Large Language Models (LLMs), prompt-learning has become
a promising method mainly researched in various research areas. Recently, many
attempts based on prompt-learning have been made to improve the performance of
text classification. However, most of these methods are based on heuristic
Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this
paper, we rethink the LLM-based text classification methodology, propose a
simple and effective transfer learning strategy, namely LLMEmbed, to address
this classical but challenging task. To illustrate, we first study how to
properly extract and fuse the text embeddings via various lightweight LLMs at
different network depths to improve their robustness and discrimination, then
adapt such embeddings to train the classifier. We perform extensive experiments
on publicly available datasets, and the results show that LLMEmbed achieves
strong performance while enjoys low training overhead using lightweight LLM
backbones compared to recent methods based on larger LLMs, i.e. GPT-3, and
sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy
on publicly available benchmarks without any fine-tuning while merely use 4%
model parameters, 1.8% electricity consumption and 1.5% runtime compared to its
counterparts. Code is available at:
https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.
</summary>
    <author>
      <name>Chun Liu</name>
    </author>
    <author>
      <name>Hongguang Zhang</name>
    </author>
    <author>
      <name>Kainan Zhao</name>
    </author>
    <author>
      <name>Xinghai Ju</name>
    </author>
    <author>
      <name>Lin Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024 main conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03749v1</id>
    <updated>2024-06-06T05:07:44Z</updated>
    <published>2024-06-06T05:07:44Z</published>
    <title>NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting
  by Learning from Human</title>
    <summary>  Increasing concerns about privacy leakage issues in academia and industry
arise when employing NLP models from third-party providers to process sensitive
texts. To protect privacy before sending sensitive data to those models, we
suggest sanitizing sensitive text using two common strategies used by humans:
i) deleting sensitive expressions, and ii) obscuring sensitive details by
abstracting them. To explore the issues and develop a tool for text rewriting,
we curate the first corpus, coined NAP^2, through both crowdsourcing and the
use of large language models (LLMs). Compared to the prior works based on
differential privacy, which lead to a sharp drop in information utility and
unnatural texts, the human-inspired approaches result in more natural rewrites
and offer an improved balance between privacy protection and data utility, as
demonstrated by our extensive experiments.
</summary>
    <author>
      <name>Shuo Huang</name>
    </author>
    <author>
      <name>William MacLean</name>
    </author>
    <author>
      <name>Xiaoxi Kang</name>
    </author>
    <author>
      <name>Anqi Wu</name>
    </author>
    <author>
      <name>Lizhen Qu</name>
    </author>
    <author>
      <name>Qiongkai Xu</name>
    </author>
    <author>
      <name>Zhuang Li</name>
    </author>
    <author>
      <name>Xingliang Yuan</name>
    </author>
    <author>
      <name>Gholamreza Haffari</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03772v1</id>
    <updated>2024-06-06T06:23:02Z</updated>
    <published>2024-06-06T06:23:02Z</published>
    <title>Character-Level Chinese Dependency Parsing via Modeling Latent
  Intra-Word Structure</title>
    <summary>  Revealing the syntactic structure of sentences in Chinese poses significant
challenges for word-level parsers due to the absence of clear word boundaries.
To facilitate a transition from word-level to character-level Chinese
dependency parsing, this paper proposes modeling latent internal structures
within words. In this way, each word-level dependency tree is interpreted as a
forest of character-level trees. A constrained Eisner algorithm is implemented
to ensure the compatibility of character-level trees, guaranteeing a single
root for intra-word structures and establishing inter-word dependencies between
these roots. Experiments on Chinese treebanks demonstrate the superiority of
our method over both the pipeline framework and previous joint models. A
detailed analysis reveals that a coarse-to-fine parsing strategy empowers the
model to predict more linguistically plausible intra-word structures.
</summary>
    <author>
      <name>Yang Hou</name>
    </author>
    <author>
      <name>Zhenghua Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03790v1</id>
    <updated>2024-06-06T07:01:50Z</updated>
    <published>2024-06-06T07:01:50Z</published>
    <title>End-to-End Trainable Soft Retriever for Low-resource Relation Extraction</title>
    <summary>  This study addresses a crucial challenge in instance-based relation
extraction using text generation models: end-to-end training in target relation
extraction task is not applicable to retrievers due to the non-differentiable
nature of instance selection. We propose a novel End-to-end TRAinable Soft
K-nearest neighbor retriever (ETRASK) by the neural prompting method that
utilizes a soft, differentiable selection of the $k$ nearest instances. This
approach enables the end-to-end training of retrievers in target tasks. On the
TACRED benchmark dataset with a low-resource setting where the training data
was reduced to 10\%, our method achieved a state-of-the-art F1 score of 71.5\%.
Moreover, ETRASK consistently improved the baseline model by adding instances
for all settings. These results highlight the efficacy of our approach in
enhancing relation extraction performance, especially in resource-constrained
environments. Our findings offer a promising direction for future research with
extraction and the broader application of text generation in natural language
processing.
</summary>
    <author>
      <name>Kohei Makino</name>
    </author>
    <author>
      <name>Makoto Miwa</name>
    </author>
    <author>
      <name>Yutaka Sasaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03792v1</id>
    <updated>2024-06-06T07:03:29Z</updated>
    <published>2024-06-06T07:03:29Z</published>
    <title>Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning</title>
    <summary>  Parameter-efficient fine-tuning (PEFT) has emerged as the predominant
technique for fine-tuning in the era of large language models. However,
existing PEFT methods still have inadequate training efficiency. Firstly, the
utilization of large-scale foundation models during the training process is
excessively redundant for certain fine-tuning tasks. Secondly, as the model
size increases, the growth in trainable parameters of empirically added PEFT
modules becomes non-negligible and redundant, leading to inefficiency. To
achieve task-specific efficient fine-tuning, we propose the Light-PEFT
framework, which includes two methods: Masked Early Pruning of the Foundation
Model and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework
allows for the simultaneous estimation of redundant parameters in both the
foundation model and PEFT modules during the early stage of training. These
parameters can then be pruned for more efficient fine-tuning. We validate our
approach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT,
parameters of the foundation model can be pruned by up to over 40%, while still
controlling trainable parameters to be only 25% of the original PEFT method.
Compared to utilizing the PEFT method directly, Light-PEFT achieves training
and inference speedup, reduces memory usage, and maintains comparable
performance and the plug-and-play feature of PEFT.
</summary>
    <author>
      <name>Naibin Gu</name>
    </author>
    <author>
      <name>Peng Fu</name>
    </author>
    <author>
      <name>Xiyu Liu</name>
    </author>
    <author>
      <name>Bowen Shen</name>
    </author>
    <author>
      <name>Zheng Lin</name>
    </author>
    <author>
      <name>Weiping Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03816v1</id>
    <updated>2024-06-06T07:40:00Z</updated>
    <published>2024-06-06T07:40:00Z</published>
    <title>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</title>
    <summary>  Recent methodologies in LLM self-training mostly rely on LLM generating
responses and filtering those with correct output answers as training data.
This approach often yields a low-quality fine-tuning training set (e.g.,
incorrect plans or intermediate reasoning). In this paper, we develop a
reinforced self-training approach, called ReST-MCTS*, based on integrating
process reward guidance with tree search MCTS* for collecting higher-quality
reasoning traces as well as per-step value to train policy and reward models.
ReST-MCTS* circumvents the per-step manual annotation typically used to train
process rewards by tree-search-based reinforcement learning: Given oracle final
correct answers, ReST-MCTS* is able to infer the correct process rewards by
estimating the probability this step can help lead to the correct answer. These
inferred rewards serve dual purposes: they act as value targets for further
refining the process reward model and also facilitate the selection of
high-quality traces for policy model self-training. We first show that the
tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior
LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same
search budget. We then show that by using traces searched by this tree-search
policy as training data, we can continuously enhance the three language models
for multiple iterations, and outperform other self-training algorithms such as
ReST$^\text{EM}$ and Self-Rewarding LM.
</summary>
    <author>
      <name>Dan Zhang</name>
    </author>
    <author>
      <name>Sining Zhoubian</name>
    </author>
    <author>
      <name>Yisong Yue</name>
    </author>
    <author>
      <name>Yuxiao Dong</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03827v1</id>
    <updated>2024-06-06T08:03:05Z</updated>
    <published>2024-06-06T08:03:05Z</published>
    <title>Chaos with Keywords: Exposing Large Language Models Sycophancy to
  Misleading Keywords and Evaluating Defense Strategies</title>
    <summary>  This study explores the sycophantic tendencies of Large Language Models
(LLMs), where these models tend to provide answers that match what users want
to hear, even if they are not entirely correct. The motivation behind this
exploration stems from the common behavior observed in individuals searching
the internet for facts with partial or misleading knowledge. Similar to using
web search engines, users may recall fragments of misleading keywords and
submit them to an LLM, hoping for a comprehensive response. Our empirical
analysis of several LLMs shows the potential danger of these models amplifying
misinformation when presented with misleading keywords. Additionally, we
thoroughly assess four existing hallucination mitigation strategies to reduce
LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of
these strategies for generating factually correct statements. Furthermore, our
analyses delve into knowledge-probing experiments on factual keywords and
different categories of sycophancy mitigation.
</summary>
    <author>
      <name>Aswin RRV</name>
    </author>
    <author>
      <name>Nemika Tyagi</name>
    </author>
    <author>
      <name>Md Nayem Uddin</name>
    </author>
    <author>
      <name>Neeraj Varshney</name>
    </author>
    <author>
      <name>Chitta Baral</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03847v2</id>
    <updated>2024-06-07T16:12:21Z</updated>
    <published>2024-06-06T08:25:43Z</published>
    <title>Lean Workbook: A large-scale Lean problem set formalized from natural
  language math problems</title>
    <summary>  Large language models have demonstrated impressive capabilities across
various natural language processing tasks, especially in solving mathematical
problems. However, large language models are not good at math theorem proving
using formal languages like Lean. A significant challenge in this area is the
scarcity of training data available in these formal languages. To address this
issue, we propose a novel pipeline that iteratively generates and filters
synthetic data to translate natural language mathematical problems into Lean 4
statements, and vice versa. Our results indicate that the synthetic data
pipeline can provide useful training data and improve the performance of LLMs
in translating and understanding complex mathematical problems and proofs. Our
final dataset contains about 57K formal-informal question pairs along with
searched proof from the math contest forum and 21 new IMO questions. We
open-source our code at https://github.com/InternLM/InternLM-Math and our data
at https://huggingface.co/datasets/InternLM/Lean-Workbook.
</summary>
    <author>
      <name>Huaiyuan Ying</name>
    </author>
    <author>
      <name>Zijian Wu</name>
    </author>
    <author>
      <name>Yihan Geng</name>
    </author>
    <author>
      <name>Jiayu Wang</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03847v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03847v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03853v1</id>
    <updated>2024-06-06T08:40:28Z</updated>
    <published>2024-06-06T08:40:28Z</published>
    <title>Speculative Decoding via Early-exiting for Faster LLM Inference with
  Thompson Sampling Control Mechanism</title>
    <summary>  The recent advancements in large language models (LLMs) have been
extraordinary, yet the escalating inference costs associated with them present
challenges in real-world applications. To address these challenges, we propose
a novel approach called Early-exiting Speculative Decoding (EESD) with lossless
acceleration. Specifically, EESD utilizes a segment of the LLM to generate
draft tokens, incorporating Early-exiting structures after the first N layers.
To enhance the quality of draft tokens, a self-distillation method is
integrated. This early-exiting design not only reduces deployment and training
costs but also significantly accelerates the token generation speed. Moreover,
we introduce a novel sampling mechanism that leverages Thompson Sampling to
regulate the generation processes, automatically determining the quantity of
draft tokens in each round. The original LLM is then employed to validate these
draft tokens through a single forward pass, and thus guarantees that the final
output text maintains a distribution consistent with vanilla auto-regressive
decoding. The experimental results on both 13B and 70B models demonstrate that
our approach decodes tokens at a markedly accelerated rate compared to prior
methods, showing the effectiveness of our approach.
</summary>
    <author>
      <name>Jiahao Liu</name>
    </author>
    <author>
      <name>Qifan Wang</name>
    </author>
    <author>
      <name>Jingang Wang</name>
    </author>
    <author>
      <name>Xunliang Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2024 (Findings)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03855v1</id>
    <updated>2024-06-06T08:41:46Z</updated>
    <published>2024-06-06T08:41:46Z</published>
    <title>Performance of large language models in numerical vs. semantic medical
  knowledge: Benchmarking on evidence-based Q&amp;As</title>
    <summary>  Clinical problem-solving requires processing of semantic medical knowledge
such as illness scripts and numerical medical knowledge of diagnostic tests for
evidence-based decision-making. As large language models (LLMs) show promising
results in many aspects of language-based clinical practice, their ability to
generate non-language evidence-based answers to clinical questions is
inherently limited by tokenization. Therefore, we evaluated LLMs' performance
on two question types: numeric (correlating findings) and semantic
(differentiating entities) while examining differences within and between LLMs
in medical aspects and comparing their performance to humans. To generate
straightforward multi-choice questions and answers (QAs) based on
evidence-based medicine (EBM), we used a comprehensive medical knowledge graph
(encompassed data from more than 50,00 peer-reviewed articles) and created the
"EBMQA". EBMQA contains 105,000 QAs labeled with medical and non-medical topics
and classified into numerical or semantic questions. We benchmarked this
dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and
Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question
types and according to sub-labeled topics. For validation, six medical experts
were tested on 100 numerical EBMQA questions. We found that both LLMs excelled
more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical
QAs. However, both LLMs showed inter and intra gaps in different medical
aspects and remained inferior to humans. Thus, their medical advice should be
addressed carefully.
</summary>
    <author>
      <name>Eden Avnat</name>
    </author>
    <author>
      <name>Michal Levy</name>
    </author>
    <author>
      <name>Daniel Herstain</name>
    </author>
    <author>
      <name>Elia Yanko</name>
    </author>
    <author>
      <name>Daniel Ben Joya</name>
    </author>
    <author>
      <name>Michal Tzuchman Katz</name>
    </author>
    <author>
      <name>Dafna Eshel</name>
    </author>
    <author>
      <name>Sahar Laros</name>
    </author>
    <author>
      <name>Yael Dagan</name>
    </author>
    <author>
      <name>Shahar Barami</name>
    </author>
    <author>
      <name>Joseph Mermelstein</name>
    </author>
    <author>
      <name>Shahar Ovadia</name>
    </author>
    <author>
      <name>Noam Shomron</name>
    </author>
    <author>
      <name>Varda Shalev</name>
    </author>
    <author>
      <name>Raja-Elie E. Abdulnour</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03869v1</id>
    <updated>2024-06-06T08:58:14Z</updated>
    <published>2024-06-06T08:58:14Z</published>
    <title>Recovering document annotations for sentence-level bitext</title>
    <summary>  Data availability limits the scope of any given task. In machine translation,
historical models were incapable of handling longer contexts, so the lack of
document-level datasets was less noticeable. Now, despite the emergence of
long-sequence methods, we remain within a sentence-level paradigm and without
data to adequately approach context-aware machine translation. Most large-scale
datasets have been processed through a pipeline that discards document-level
metadata. In this work, we reconstruct document-level information for three
(ParaCrawl, News Commentary, and Europarl) large datasets in German, French,
Spanish, Italian, Polish, and Portuguese (paired with English). We then
introduce a document-level filtering technique as an alternative to traditional
bitext filtering. We present this filtering with analysis to show that this
method prefers context-consistent translations rather than those that may have
been sentence-level machine translated. Last we train models on these longer
contexts and demonstrate improvement in document-level translation without
degradation of sentence-level translation. We release our dataset, ParaDocs,
and resulting models as a resource to the community.
</summary>
    <author>
      <name>Rachel Wicks</name>
    </author>
    <author>
      <name>Matt Post</name>
    </author>
    <author>
      <name>Philipp Koehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2024 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03878v1</id>
    <updated>2024-06-06T09:13:13Z</updated>
    <published>2024-06-06T09:13:13Z</published>
    <title>Decoder-only Streaming Transformer for Simultaneous Translation</title>
    <summary>  Simultaneous Machine Translation (SiMT) generates translation while reading
source tokens, essentially producing the target prefix based on the source
prefix. To achieve good performance, it leverages the relationship between
source and target prefixes to exact a policy to guide the generation of
translations. Although existing SiMT methods primarily focus on the
Encoder-Decoder architecture, we explore the potential of Decoder-only
architecture, owing to its superior performance in various tasks and its
inherent compatibility with SiMT. However, directly applying the Decoder-only
architecture to SiMT poses challenges in terms of training and inference. To
alleviate the above problems, we propose the first Decoder-only SiMT model,
named Decoder-only Streaming Transformer (DST). Specifically, DST separately
encodes the positions of the source and target prefixes, ensuring that the
position of the target prefix remains unaffected by the expansion of the source
prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism
tailored for the Decoder-only architecture. It is capable of obtaining
translation policy by assessing the sufficiency of input source information and
integrating with the soft-attention mechanism to generate translations.
Experiments demonstrate that our approach achieves state-of-the-art performance
on three translation tasks.
</summary>
    <author>
      <name>Shoutao Guo</name>
    </author>
    <author>
      <name>Shaolei Zhang</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024. 14 pages, 10 Tables, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03881v1</id>
    <updated>2024-06-06T09:18:42Z</updated>
    <published>2024-06-06T09:18:42Z</published>
    <title>Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations,
  Automatic Metrics, and Segmentation</title>
    <summary>  Human evaluation is a critical component in machine translation system
development and has received much attention in text translation research.
However, little prior work exists on the topic of human evaluation for speech
translation, which adds additional challenges such as noisy data and
segmentation mismatches. We take first steps to fill this gap by conducting a
comprehensive human evaluation of the results of several shared tasks from the
last International Workshop on Spoken Language Translation (IWSLT 2023). We
propose an effective evaluation strategy based on automatic resegmentation and
direct assessment with segment context. Our analysis revealed that: 1) the
proposed evaluation strategy is robust and scores well-correlated with other
types of human judgements; 2) automatic metrics are usually, but not always,
well-correlated with direct assessment scores; and 3) COMET as a slightly
stronger automatic metric than chrF, despite the segmentation noise introduced
by the resegmentation step systems. We release the collected human-annotated
data in order to encourage further investigation.
</summary>
    <author>
      <name>Matthias Sperber</name>
    </author>
    <author>
      <name>Ondřej Bojar</name>
    </author>
    <author>
      <name>Barry Haddow</name>
    </author>
    <author>
      <name>Dávid Javorský</name>
    </author>
    <author>
      <name>Xutai Ma</name>
    </author>
    <author>
      <name>Matteo Negri</name>
    </author>
    <author>
      <name>Jan Niehues</name>
    </author>
    <author>
      <name>Peter Polák</name>
    </author>
    <author>
      <name>Elizabeth Salesky</name>
    </author>
    <author>
      <name>Katsuhito Sudoh</name>
    </author>
    <author>
      <name>Marco Turchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LREC-COLING2024 publication (with corrections for Table 3)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2024 Joint International Conference on
  Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2406.03881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03893v1</id>
    <updated>2024-06-06T09:28:08Z</updated>
    <published>2024-06-06T09:28:08Z</published>
    <title>How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?</title>
    <summary>  While machine translation evaluation has been studied primarily for
high-resource languages, there has been a recent interest in evaluation for
low-resource languages due to the increasing availability of data and models.
In this paper, we focus on a zero-shot evaluation setting focusing on
low-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi.
We collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct
Assessment (DA) annotations to create test sets and meta-evaluate a plethora of
automatic evaluation metrics. We observe that even for learned metrics, which
are known to exhibit zero-shot performance, the Kendall Tau and Pearson
correlations with human annotations are only as high as 0.32 and 0.45.
Synthetic data approaches show mixed results and overall do not help close the
gap by much for these languages. This indicates that there is still a long way
to go for low-resource evaluation.
</summary>
    <author>
      <name>Anushka Singh</name>
    </author>
    <author>
      <name>Ananya B. Sai</name>
    </author>
    <author>
      <name>Raj Dabre</name>
    </author>
    <author>
      <name>Ratish Puduppully</name>
    </author>
    <author>
      <name>Anoop Kunchukuttan</name>
    </author>
    <author>
      <name>Mitesh M Khapra</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03930v1</id>
    <updated>2024-06-06T10:16:43Z</updated>
    <published>2024-06-06T10:16:43Z</published>
    <title>Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State
  of the Art</title>
    <summary>  The surge of interest in culturally aware and adapted Natural Language
Processing (NLP) has inspired much recent research. However, the lack of common
understanding of the concept of "culture" has made it difficult to evaluate
progress in this emerging area. Drawing on prior research in NLP and related
fields, we propose an extensive taxonomy of elements of culture that can
provide a systematic framework for analyzing and understanding research
progress. Using the taxonomy, we survey existing resources and models for
culturally aware and adapted NLP, providing an overview of the state of the art
and the research gaps that still need to be filled.
</summary>
    <author>
      <name>Chen Cecilia Liu</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <author>
      <name>Anna Korhonen</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03949v1</id>
    <updated>2024-06-06T10:50:26Z</updated>
    <published>2024-06-06T10:50:26Z</published>
    <title>UltraMedical: Building Specialized Generalists in Biomedicine</title>
    <summary>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains and are moving towards more specialized areas. Recent advanced
proprietary models such as GPT-4 and Gemini have achieved significant
advancements in biomedicine, which have also raised privacy and security
challenges. The construction of specialized generalists hinges largely on
high-quality datasets, enhanced by techniques like supervised fine-tuning and
reinforcement learning from human or AI feedback, and direct preference
optimization. However, these leading technologies (e.g., preference learning)
are still significantly limited in the open source community due to the
scarcity of specialized data. In this paper, we present the UltraMedical
collections, which consist of high-quality manual and synthetic datasets in the
biomedicine domain, featuring preference annotations across multiple advanced
LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical
models based on Llama-3 series, demonstrating breathtaking capabilities across
various medical benchmarks. Moreover, we develop powerful reward models skilled
in biomedical and general reward benchmark, enhancing further online preference
learning within the biomedical LLM community.
</summary>
    <author>
      <name>Kaiyan Zhang</name>
    </author>
    <author>
      <name>Sihang Zeng</name>
    </author>
    <author>
      <name>Ermo Hua</name>
    </author>
    <author>
      <name>Ning Ding</name>
    </author>
    <author>
      <name>Zhang-Ren Chen</name>
    </author>
    <author>
      <name>Zhiyuan Ma</name>
    </author>
    <author>
      <name>Haoxin Li</name>
    </author>
    <author>
      <name>Ganqu Cui</name>
    </author>
    <author>
      <name>Biqing Qi</name>
    </author>
    <author>
      <name>Xuekai Zhu</name>
    </author>
    <author>
      <name>Xingtai Lv</name>
    </author>
    <author>
      <name>Hu Jinfang</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Datasets and models are available at
  https://github.com/TsinghuaC3I/UltraMedical</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03953v1</id>
    <updated>2024-06-06T10:54:44Z</updated>
    <published>2024-06-06T10:54:44Z</published>
    <title>Tox-BART: Leveraging Toxicity Attributes for Explanation Generation of
  Implicit Hate Speech</title>
    <summary>  Employing language models to generate explanations for an incoming implicit
hate post is an active area of research. The explanation is intended to make
explicit the underlying stereotype and aid content moderators. The training
often combines top-k relevant knowledge graph (KG) tuples to provide world
knowledge and improve performance on standard metrics. Interestingly, our study
presents conflicting evidence for the role of the quality of KG tuples in
generating implicit explanations. Consequently, simpler models incorporating
external toxicity signals outperform KG-infused models. Compared to the
KG-based setup, we observe a comparable performance for SBIC (LatentHatred)
datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and
-4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and
error analysis reveal that our proposed setup produces more precise
explanations than zero-shot GPT-3.5, highlighting the intricate nature of the
task.
</summary>
    <author>
      <name>Neemesh Yadav</name>
    </author>
    <author>
      <name>Sarah Masud</name>
    </author>
    <author>
      <name>Vikram Goyal</name>
    </author>
    <author>
      <name>Vikram Goyal</name>
    </author>
    <author>
      <name>Md Shad Akhtar</name>
    </author>
    <author>
      <name>Tanmoy Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, 5 Figures, 13 Tables, ACL Findings 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03963v1</id>
    <updated>2024-06-06T11:14:27Z</updated>
    <published>2024-06-06T11:14:27Z</published>
    <title>A + B: A General Generator-Reader Framework for Optimizing LLMs to
  Unleash Synergy Potential</title>
    <summary>  Retrieval-Augmented Generation (RAG) is an effective solution to supplement
necessary knowledge to large language models (LLMs). Targeting its bottleneck
of retriever performance, "generate-then-read" pipeline is proposed to replace
the retrieval stage with generation from the LLM itself. Although promising,
this research direction is underexplored and still cannot work in the scenario
when source knowledge is given. In this paper, we formalize a general "A + B"
framework with varying combinations of foundation models and types for
systematic investigation. We explore the efficacy of the base and chat versions
of LLMs and found their different functionalities suitable for generator A and
reader B, respectively. Their combinations consistently outperform single
models, especially in complex scenarios. Furthermore, we extend the application
of the "A + B" framework to scenarios involving source documents through
continuous learning, enabling the direct integration of external knowledge into
LLMs. This approach not only facilitates effective acquisition of new knowledge
but also addresses the challenges of safety and helpfulness post-adaptation.
The paper underscores the versatility of the "A + B" framework, demonstrating
its potential to enhance the practical application of LLMs across various
domains.
</summary>
    <author>
      <name>Wei Tang</name>
    </author>
    <author>
      <name>Yixin Cao</name>
    </author>
    <author>
      <name>Jiahao Ying</name>
    </author>
    <author>
      <name>Bo Wang</name>
    </author>
    <author>
      <name>Yuyue Zhao</name>
    </author>
    <author>
      <name>Yong Liao</name>
    </author>
    <author>
      <name>Pengyuan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL'24 (Findings)</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.03963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.03993v1</id>
    <updated>2024-06-06T12:08:43Z</updated>
    <published>2024-06-06T12:08:43Z</published>
    <title>Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens
  of Relevance Paraphrasing</title>
    <summary>  Large Language Models (LLMs) have achieved state-of-the-art performance at
zero-shot generation of abstractive summaries for given articles. However,
little is known about the robustness of such a process of zero-shot
summarization. To bridge this gap, we propose relevance paraphrasing, a simple
strategy that can be used to measure the robustness of LLMs as summarizers. The
relevance paraphrasing approach identifies the most relevant sentences that
contribute to generating an ideal summary, and then paraphrases these inputs to
obtain a minimally perturbed dataset. Then, by evaluating model performance for
summarization on both the original and perturbed datasets, we can assess the
LLM's one aspect of robustness. We conduct extensive experiments with relevance
paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes
(GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B). Our results indicate
that LLMs are not consistent summarizers for the minimally perturbed articles,
necessitating further improvements.
</summary>
    <author>
      <name>Hadi Askari</name>
    </author>
    <author>
      <name>Anshuman Chhabra</name>
    </author>
    <author>
      <name>Muhao Chen</name>
    </author>
    <author>
      <name>Prasant Mohapatra</name>
    </author>
    <link href="http://arxiv.org/abs/2406.03993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.03993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04025v1</id>
    <updated>2024-06-06T12:51:14Z</updated>
    <published>2024-06-06T12:51:14Z</published>
    <title>The syntax-semantics interface in a child's path: A study of 3- to
  11-year-olds' elicited production of Mandarin recursive relative clauses</title>
    <summary>  There have been apparently conflicting claims over the syntax-semantics
relationship in child acquisition. However, few of them have assessed the
child's path toward the acquisition of recursive relative clauses (RRCs). The
authors of the current paper did experiments to investigate 3- to 11-year-olds'
most-structured elicited production of eight Mandarin RRCs in a 4 (syntactic
types)*2 (semantic conditions) design. The four syntactic types were RRCs with
a subject-gapped RC embedded in an object-gapped RC (SORRCs), RRCs with an
object-gapped RC embedded in another object-gapped RC (OORRCs), RRCs with an
object-gapped RC embedded in a subject-gapped RC (OSRRCs), and RRCs with a
subject-gapped RC embedded in another subject-gapped RC (SSRRCs). Each
syntactic type was put in two conditions differing in internal semantics:
irreversible internal semantics (IIS) and reversible internal semantics (RIS).
For example, "the balloon that [the girl that _ eats the banana] holds _" is
SORRCs in the IIS condition; "the monkey that [the dog that _ bites the pig]
hits_" is SORRCs in the RIS condition. For each target, the participants were
provided with a speech-visual stimulus constructing a condition of irreversible
external semantics (IES). The results showed that SSRRCs, OSRRCs and SORRCs in
the IIS-IES condition were produced two years earlier than their counterparts
in the RIS-IES condition. Thus, a 2-stage development path is proposed: the
language acquisition device starts with the interface between (irreversible)
syntax and IIS, and ends with the interface between syntax and IES, both
abiding by the syntax-semantic interface principle.
</summary>
    <author>
      <name>Caimei Yang</name>
    </author>
    <author>
      <name>Qihang Yang</name>
    </author>
    <author>
      <name>Xingzhi Su</name>
    </author>
    <author>
      <name>Chenxi Fu</name>
    </author>
    <author>
      <name>Xiaoyi Wang</name>
    </author>
    <author>
      <name>Ying Yan</name>
    </author>
    <author>
      <name>Zaijiang Man</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04106v1</id>
    <updated>2024-06-06T14:23:10Z</updated>
    <published>2024-06-06T14:23:10Z</published>
    <title>Explainability and Hate Speech: Structured Explanations Make Social
  Media Moderators Faster</title>
    <summary>  Content moderators play a key role in keeping the conversation on social
media healthy. While the high volume of content they need to judge represents a
bottleneck to the moderation pipeline, no studies have explored how models
could support them to make faster decisions. There is, by now, a vast body of
research into detecting hate speech, sometimes explicitly motivated by a desire
to help improve content moderation, but published research using real content
moderators is scarce. In this work we investigate the effect of explanations on
the speed of real-world moderators. Our experiments show that while generic
explanations do not affect their speed and are often ignored, structured
explanations lower moderators' decision making time by 7.4%.
</summary>
    <author>
      <name>Agostina Calabrese</name>
    </author>
    <author>
      <name>Leonardo Neves</name>
    </author>
    <author>
      <name>Neil Shah</name>
    </author>
    <author>
      <name>Maarten W. Bos</name>
    </author>
    <author>
      <name>Björn Ross</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <author>
      <name>Francesco Barbieri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 14 figures, to be published at ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04109v1</id>
    <updated>2024-06-06T14:26:35Z</updated>
    <published>2024-06-06T14:26:35Z</published>
    <title>Intention and Face in Dialog</title>
    <summary>  The notion of face described by Brown and Levinson (1987) has been studied in
great detail, but a critical aspect of the framework, that which focuses on how
intentions mediate the planning of turns which impose upon face, has received
far less attention. We present an analysis of three computational systems
trained for classifying both intention and politeness, focusing on how the
former influences the latter. In politeness theory, agents attend to the desire
to have their wants appreciated (positive face), and a complementary desire to
act unimpeded and maintain freedom (negative face). Similar to speech acts,
utterances can perform so-called face acts which can either raise or threaten
the positive or negative face of the speaker or hearer. We begin by using an
existing corpus to train a model which classifies face acts, achieving a new
SoTA in the process. We then observe that every face act has an underlying
intention that motivates it and perform additional experiments integrating
dialog act annotations to provide these intentions by proxy. Our analysis finds
that dialog acts improve performance on face act detection for minority classes
and points to a close relationship between aspects of face and intent.
</summary>
    <author>
      <name>Adil Soubki</name>
    </author>
    <author>
      <name>Owen Rambow</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">May 2024. In Proceedings of the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024), pages 9143-9153, Torino, Italia. ELRA and ICCL</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2406.04109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04113v1</id>
    <updated>2024-06-06T14:30:59Z</updated>
    <published>2024-06-06T14:30:59Z</published>
    <title>Uncovering Limitations of Large Language Models in Information Seeking
  from Tables</title>
    <summary>  Tables are recognized for their high information density and widespread
usage, serving as essential sources of information. Seeking information from
tables (TIS) is a crucial capability for Large Language Models (LLMs), serving
as the foundation of knowledge-based Q&amp;A systems. However, this field presently
suffers from an absence of thorough and reliable evaluation. This paper
introduces a more reliable benchmark for Table Information Seeking (TabIS). To
avoid the unreliable evaluation caused by text similarity-based metrics, TabIS
adopts a single-choice question format (with two options per question) instead
of a text generation format. We establish an effective pipeline for generating
options, ensuring their difficulty and quality. Experiments conducted on 12
LLMs reveal that while the performance of GPT-4-turbo is marginally
satisfactory, both other proprietary and open-source models perform
inadequately. Further analysis shows that LLMs exhibit a poor understanding of
table structures, and struggle to balance between TIS performance and
robustness against pseudo-relevant tables (common in retrieval-augmented
systems). These findings uncover the limitations and potential challenges of
LLMs in seeking information from tables. We release our data and code to
facilitate further research in this field.
</summary>
    <author>
      <name>Chaoxu Pang</name>
    </author>
    <author>
      <name>Yixuan Cao</name>
    </author>
    <author>
      <name>Chunhao Yang</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04146v1</id>
    <updated>2024-06-06T15:11:11Z</updated>
    <published>2024-06-06T15:11:11Z</published>
    <title>Towards Understanding Task-agnostic Debiasing Through the Lenses of
  Intrinsic Bias and Forgetfulness</title>
    <summary>  While task-agnostic debiasing provides notable generalizability and reduced
reliance on downstream data, its impact on language modeling ability and the
risk of relearning social biases from downstream task-specific data remain as
the two most significant challenges when debiasing Pretrained Language Models
(PLMs). The impact on language modeling ability can be alleviated given a
high-quality and long-contextualized debiasing corpus, but there remains a
deficiency in understanding the specifics of relearning biases. We empirically
ascertain that the effectiveness of task-agnostic debiasing hinges on the
quantitative bias level of both the task-specific data used for downstream
applications and the debiased model. We empirically show that the lower bound
of the bias level of the downstream fine-tuned model can be approximated by the
bias level of the debiased model, in most practical cases. To gain more
in-depth understanding about how the parameters of PLMs change during
fine-tuning due to the forgetting issue of PLMs, we propose a novel framework
which can Propagate Socially-fair Debiasing to Downstream Fine-tuning,
ProSocialTuning. Our proposed framework can push the fine-tuned model to
approach the bias lower bound during downstream fine-tuning, indicating that
the ineffectiveness of debiasing can be alleviated by overcoming the forgetting
issue through regularizing successfully debiased attention heads based on the
PLMs' bias levels from stages of pretraining and debiasing.
</summary>
    <author>
      <name>Guangliang Liu</name>
    </author>
    <author>
      <name>Milad Afshari</name>
    </author>
    <author>
      <name>Xitong Zhang</name>
    </author>
    <author>
      <name>Zhiyu Xue</name>
    </author>
    <author>
      <name>Avrajit Ghosh</name>
    </author>
    <author>
      <name>Bidhan Bashyal</name>
    </author>
    <author>
      <name>Rongrong Wang</name>
    </author>
    <author>
      <name>Kristen Johnson</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04197v1</id>
    <updated>2024-06-06T15:55:53Z</updated>
    <published>2024-06-06T15:55:53Z</published>
    <title>DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase
  for Math Reasoning</title>
    <summary>  The advancement of large language models (LLMs) relies on evaluation using
public benchmarks, but data contamination can lead to overestimated
performance. Previous researches focus on detecting contamination by
determining whether the model has seen the exact same data during training. In
this work, we argue that even training on data similar to benchmark data
inflates performance on in-distribution tasks without improving overall
capacity, which we called In-distribution contamination. To effectively detect
in-distribution contamination, we propose DICE, a novel method that leverages
the internal states of LLMs to locate-then-detect the contamination. DICE first
identifies the most sensitive layer to contamination, then trains a classifier
based on the internal states of that layer. Experiments reveal DICE's high
accuracy in detecting in-distribution contamination across various LLMs and
math reasoning datasets. We also show the generalization capability of the
trained DICE detector, which is able to detect contamination across multiple
benchmarks with similar distributions. Additionally, we find that the DICE
detection scores are positively correlated with the performance of ten LLMs
fine-tuned by either us or other organizations on four math reasoning datasets
(with $R^2$ values between 0.6 and 0.75). This indicates that the
in-distribution contamination problem potentially lead to an overestimation of
the true capabilities of many existing models. The code and data are available
at https://github.com/THU-KEG/DICE.
</summary>
    <author>
      <name>Shangqing Tu</name>
    </author>
    <author>
      <name>Kejian Zhu</name>
    </author>
    <author>
      <name>Yushi Bai</name>
    </author>
    <author>
      <name>Zijun Yao</name>
    </author>
    <author>
      <name>Lei Hou</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04214v1</id>
    <updated>2024-06-06T16:14:16Z</updated>
    <published>2024-06-06T16:14:16Z</published>
    <title>ValueBench: Towards Comprehensively Evaluating Value Orientations and
  Understanding of Large Language Models</title>
    <summary>  Large Language Models (LLMs) are transforming diverse fields and gaining
increasing influence as human proxies. This development underscores the urgent
need for evaluating value orientations and understanding of LLMs to ensure
their responsible integration into public-facing applications. This work
introduces ValueBench, the first comprehensive psychometric benchmark for
evaluating value orientations and value understanding in LLMs. ValueBench
collects data from 44 established psychometric inventories, encompassing 453
multifaceted value dimensions. We propose an evaluation pipeline grounded in
realistic human-AI interactions to probe value orientations, along with novel
tasks for evaluating value understanding in an open-ended value space. With
extensive experiments conducted on six representative LLMs, we unveil their
shared and distinctive value orientations and exhibit their ability to
approximate expert conclusions in value-related extraction and generation
tasks. ValueBench is openly accessible at
https://github.com/Value4AI/ValueBench.
</summary>
    <author>
      <name>Yuanyi Ren</name>
    </author>
    <author>
      <name>Haoran Ye</name>
    </author>
    <author>
      <name>Hanjun Fang</name>
    </author>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Guojie Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04218v1</id>
    <updated>2024-06-06T16:18:02Z</updated>
    <published>2024-06-06T16:18:02Z</published>
    <title>Rethinking LLM and Linguistic Steganalysis: An Efficient Detection of
  Strongly Concealed Stego</title>
    <summary>  To detect stego (steganographic text) in complex scenarios, linguistic
steganalysis (LS) with various motivations has been proposed and achieved
excellent performance. However, with the development of generative
steganography, some stegos have strong concealment, especially after the
emergence of LLMs-based steganography, the existing LS has low detection or
even cannot detect them. We designed a novel LS with two modes called LSGC. In
the generation mode, we created an LS-task "description" and used the
generation ability of LLM to explain whether texts to be detected are stegos.
On this basis, we rethought the principle of LS and LLMs, and proposed the
classification mode. In this mode, LSGC deleted the LS-task "description" and
changed the "causalLM" LLMs to the "sequenceClassification" architecture. The
LS features can be extracted by only one pass of the model, and a linear layer
with initialization weights is added to obtain the classification probability.
Experiments on strongly concealed stegos show that LSGC significantly improves
detection and reaches SOTA performance. Additionally, LSGC in classification
mode greatly reduces training time while maintaining high performance.
</summary>
    <author>
      <name>Yifan Tang</name>
    </author>
    <author>
      <name>Yihao Wang</name>
    </author>
    <author>
      <name>Ru Zhang</name>
    </author>
    <author>
      <name>Jianyi Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04244v1</id>
    <updated>2024-06-06T16:41:39Z</updated>
    <published>2024-06-06T16:41:39Z</published>
    <title>Benchmark Data Contamination of Large Language Models: A Survey</title>
    <summary>  The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3,
and Gemini has transformed the field of natural language processing. However,
it has also resulted in a significant issue known as Benchmark Data
Contamination (BDC). This occurs when language models inadvertently incorporate
evaluation benchmark information from their training data, leading to
inaccurate or unreliable performance during the evaluation phase of the
process. This paper reviews the complex challenge of BDC in LLM evaluation and
explores alternative assessment methods to mitigate the risks associated with
traditional benchmarks. The paper also examines challenges and future
directions in mitigating BDC risks, highlighting the complexity of the issue
and the need for innovative solutions to ensure the reliability of LLM
evaluation in real-world applications.
</summary>
    <author>
      <name>Cheng Xu</name>
    </author>
    <author>
      <name>Shuhao Guan</name>
    </author>
    <author>
      <name>Derek Greene</name>
    </author>
    <author>
      <name>M-Tahar Kechadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04271v1</id>
    <updated>2024-06-06T17:22:08Z</updated>
    <published>2024-06-06T17:22:08Z</published>
    <title>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language
  Models</title>
    <summary>  We introduce Buffer of Thoughts (BoT), a novel and versatile
thought-augmented reasoning approach for enhancing accuracy, efficiency and
robustness of large language models (LLMs). Specifically, we propose
meta-buffer to store a series of informative high-level thoughts, namely
thought-template, distilled from the problem-solving processes across various
tasks. Then for each problem, we retrieve a relevant thought-template and
adaptively instantiate it with specific reasoning structures to conduct
efficient reasoning. To guarantee the scalability and stability, we further
propose buffer-manager to dynamically update the meta-buffer, thus enhancing
the capacity of meta-buffer as more tasks are solved. We conduct extensive
experiments on 10 challenging reasoning-intensive tasks, and achieve
significant performance improvements over previous SOTA methods: 11% on Game of
24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis
demonstrate the superior generalization ability and model robustness of our
BoT, while requiring only 12% of the cost of multi-query prompting methods
(e.g., tree/graph of thoughts) on average. Notably, we find that our
Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is
available at: https://github.com/YangLing0818/buffer-of-thought-llm
</summary>
    <author>
      <name>Ling Yang</name>
    </author>
    <author>
      <name>Zhaochen Yu</name>
    </author>
    <author>
      <name>Tianjun Zhang</name>
    </author>
    <author>
      <name>Shiyi Cao</name>
    </author>
    <author>
      <name>Minkai Xu</name>
    </author>
    <author>
      <name>Wentao Zhang</name>
    </author>
    <author>
      <name>Joseph E. Gonzalez</name>
    </author>
    <author>
      <name>Bin Cui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project: https://github.com/YangLing0818/buffer-of-thought-llm</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04289v2</id>
    <updated>2024-06-07T08:30:02Z</updated>
    <published>2024-06-06T17:34:24Z</published>
    <title>What Languages are Easy to Language-Model? A Perspective from Learning
  Probabilistic Regular Languages</title>
    <summary>  What can large language models learn? By definition, language models (LM) are
distributions over strings. Therefore, an intuitive way of addressing the above
question is to formalize it as a matter of learnability of classes of
distributions over strings. While prior work in this direction focused on
assessing the theoretical limits, in contrast, we seek to understand the
empirical learnability. Unlike prior empirical work, we evaluate neural LMs on
their home turf-learning probabilistic languages-rather than as classifiers of
formal languages. In particular, we investigate the learnability of regular LMs
(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs
as a function of various complexity parameters of the RLM and the hidden state
size of the neural LM. We find that the RLM rank, which corresponds to the size
of linear space spanned by the logits of its conditional distributions, and the
expected length of sampled strings are strong and significant predictors of
learnability for both RNNs and Transformers. Several other predictors also
reach significance, but with differing patterns between RNNs and Transformers.
</summary>
    <author>
      <name>Nadav Borenstein</name>
    </author>
    <author>
      <name>Anej Svete</name>
    </author>
    <author>
      <name>Robin Chan</name>
    </author>
    <author>
      <name>Josef Valvoda</name>
    </author>
    <author>
      <name>Franz Nowak</name>
    </author>
    <author>
      <name>Isabelle Augenstein</name>
    </author>
    <author>
      <name>Eleanor Chodroff</name>
    </author>
    <author>
      <name>Ryan Cotterell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04289v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04289v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04460v1</id>
    <updated>2024-06-06T19:35:51Z</updated>
    <published>2024-06-06T19:35:51Z</published>
    <title>Evaluating the Smooth Control of Attribute Intensity in Text Generation
  with LLMs</title>
    <summary>  Controlling the attribute intensity of text generation is crucial across
scenarios (e.g., writing conciseness, chatting emotion, and explanation
clarity). The remarkable capabilities of large language models (LLMs) have
revolutionized text generation, prompting us to explore such \emph{smooth
control} of LLM generation. Specifically, we propose metrics to assess the
range, calibration, and consistency of the generated text's attribute intensity
in response to varying control values, as well as its relevance to the intended
context. To quantify the attribute intensity and context relevance, we propose
an effective evaluation framework leveraging the Elo rating system and GPT4,
both renowned for their robust alignment with human judgment. We look into two
viable training-free methods for achieving smooth control of LLMs: (1)
Prompting with semantic shifters, and (2) Modifying internal model
representations. The evaluations of these two methods are conducted on $5$
different attributes with various models. Our code and dataset can be obtained
from \url{https://github.com/ShangDataLab/Smooth-Control}.
</summary>
    <author>
      <name>Shang Zhou</name>
    </author>
    <author>
      <name>Feng Yao</name>
    </author>
    <author>
      <name>Chengyu Dong</name>
    </author>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Jingbo Shang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2024 Findings</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04461v1</id>
    <updated>2024-06-06T19:37:25Z</updated>
    <published>2024-06-06T19:37:25Z</published>
    <title>Multi-Label Classification for Implicit Discourse Relation Recognition</title>
    <summary>  Discourse relations play a pivotal role in establishing coherence within
textual content, uniting sentences and clauses into a cohesive narrative. The
Penn Discourse Treebank (PDTB) stands as one of the most extensively utilized
datasets in this domain. In PDTB-3, the annotators can assign multiple labels
to an example, when they believe that multiple relations are present. Prior
research in discourse relation recognition has treated these instances as
separate examples during training, and only one example needs to have its label
predicted correctly for the instance to be judged as correct. However, this
approach is inadequate, as it fails to account for the interdependence of
labels in real-world contexts and to distinguish between cases where only one
sense relation holds and cases where multiple relations hold simultaneously. In
our work, we address this challenge by exploring various multi-label
classification frameworks to handle implicit discourse relation recognition. We
show that multi-label classification methods don't depress performance for
single-label prediction. Additionally, we give comprehensive analysis of
results and data. Our work contributes to advancing the understanding and
application of discourse relations and provide a foundation for the future
study
</summary>
    <author>
      <name>Wanqiu Long</name>
    </author>
    <author>
      <name>N. Siddharth</name>
    </author>
    <author>
      <name>Bonnie Webber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL2024 Finding</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04528v1</id>
    <updated>2024-06-06T22:01:59Z</updated>
    <published>2024-06-06T22:01:59Z</published>
    <title>llmNER: (Zero|Few)-Shot Named Entity Recognition, Exploiting the Power
  of Large Language Models</title>
    <summary>  Large language models (LLMs) allow us to generate high-quality human-like
text. One interesting task in natural language processing (NLP) is named entity
recognition (NER), which seeks to detect mentions of relevant information in
documents. This paper presents llmNER, a Python library for implementing
zero-shot and few-shot NER with LLMs; by providing an easy-to-use interface,
llmNER can compose prompts, query the model, and parse the completion returned
by the LLM. Also, the library enables the user to perform prompt engineering
efficiently by providing a simple interface to test multiple variables. We
validated our software on two NER tasks to show the library's flexibility.
llmNER aims to push the boundaries of in-context learning research by removing
the barrier of the prompting and parsing steps.
</summary>
    <author>
      <name>Fabián Villena</name>
    </author>
    <author>
      <name>Luis Miranda</name>
    </author>
    <author>
      <name>Claudio Aracena</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04578v1</id>
    <updated>2024-06-07T01:43:07Z</updated>
    <published>2024-06-07T01:43:07Z</published>
    <title>SC2: Towards Enhancing Content Preservation and Style Consistency in
  Long Text Style Transfer</title>
    <summary>  Text style transfer (TST) aims to vary the style polarity of text while
preserving the semantic content. Although recent advancements have demonstrated
remarkable progress in short TST, it remains a relatively straightforward task
with limited practical applications. The more comprehensive long TST task
presents two challenges: (1) existing methods encounter difficulties in
accurately evaluating content attributes in multiple words, leading to content
degradation; (2) the conventional vanilla style classifier loss encounters
obstacles in maintaining consistent style across multiple generated sentences.
  In this paper, we propose a novel method SC2, where a multilayer Joint
Style-Content Weighed (JSCW) module and a Style Consistency loss are designed
to address the two issues. The JSCW simultaneously assesses the amounts of
style and content attributes within a token, aiming to acquire a lossless
content representation and thereby enhancing content preservation. The multiple
JSCW layers further progressively refine content representations. We design a
style consistency loss to ensure the generated multiple sentences consistently
reflect the target style polarity. Moreover, we incorporate a denoising
non-autoregressive decoder to accelerate the training. We conduct plentiful
experiments and the results show significant improvements of SC2 over
competitive baselines. Our code: https://github.com/jiezhao6/SC2.
</summary>
    <author>
      <name>Jie Zhao</name>
    </author>
    <author>
      <name>Ziyu Guan</name>
    </author>
    <author>
      <name>Cai Xu</name>
    </author>
    <author>
      <name>Wei Zhao</name>
    </author>
    <author>
      <name>Yue Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04583v1</id>
    <updated>2024-06-07T02:11:49Z</updated>
    <published>2024-06-07T02:11:49Z</published>
    <title>Extroversion or Introversion? Controlling The Personality of Your Large
  Language Models</title>
    <summary>  Large language models (LLMs) exhibit robust capabilities in text generation
and comprehension, mimicking human behavior and exhibiting synthetic
personalities. However, some LLMs have displayed offensive personality,
propagating toxic discourse. Existing literature neglects the origin and
evolution of LLM personalities, as well as the effective personality control.
To fill these gaps, our study embarked on a comprehensive investigation into
LLM personality control. We investigated several typical methods to influence
LLMs, including three training methods: Continual Pre-training, Supervised
Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along
with inference phase considerations (prompts). Our investigation revealed a
hierarchy of effectiveness in control: Prompt &gt; SFT &gt; RLHF &gt; Continual
Pre-train. Notably, SFT exhibits a higher control success rate compared to
prompt induction. While prompts prove highly effective, we found that
prompt-induced personalities are less robust than those trained, making them
more prone to showing conflicting personalities under reverse personality
prompt induction. Besides, harnessing the strengths of both SFT and prompt, we
proposed $\underline{\text{P}}$rompt $\underline{\text{I}}$nduction post
$\underline{\text{S}}$upervised $\underline{\text{F}}$ine-tuning (PISF), which
emerges as the most effective and robust strategy for controlling LLMs'
personality, displaying high efficacy, high success rates, and high robustness.
Even under reverse personality prompt induction, LLMs controlled by PISF still
exhibit stable and robust personalities.
</summary>
    <author>
      <name>Yanquan Chen</name>
    </author>
    <author>
      <name>Zhen Wu</name>
    </author>
    <author>
      <name>Junjie Guo</name>
    </author>
    <author>
      <name>Shujian Huang</name>
    </author>
    <author>
      <name>Xinyu Dai</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04630v1</id>
    <updated>2024-06-07T04:31:41Z</updated>
    <published>2024-06-07T04:31:41Z</published>
    <title>Low-Resource Cross-Lingual Summarization through Few-Shot Learning with
  Large Language Models</title>
    <summary>  Cross-lingual summarization (XLS) aims to generate a summary in a target
language different from the source language document. While large language
models (LLMs) have shown promising zero-shot XLS performance, their few-shot
capabilities on this task remain unexplored, especially for low-resource
languages with limited parallel data. In this paper, we investigate the
few-shot XLS performance of various models, including Mistral-7B-Instruct-v0.2,
GPT-3.5, and GPT-4. Our experiments demonstrate that few-shot learning
significantly improves the XLS performance of LLMs, particularly GPT-3.5 and
GPT-4, in low-resource settings. However, the open-source model
Mistral-7B-Instruct-v0.2 struggles to adapt effectively to the XLS task with
limited examples. Our findings highlight the potential of few-shot learning for
improving XLS performance and the need for further research in designing LLM
architectures and pre-training objectives tailored for this task. We provide a
future work direction to explore more effective few-shot learning strategies
and to investigate the transfer learning capabilities of LLMs for cross-lingual
summarization.
</summary>
    <author>
      <name>Gyutae Park</name>
    </author>
    <author>
      <name>Seojin Hwang</name>
    </author>
    <author>
      <name>Hwanhee Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04638v1</id>
    <updated>2024-06-07T04:52:46Z</updated>
    <published>2024-06-07T04:52:46Z</published>
    <title>Large Language Model-guided Document Selection</title>
    <summary>  Large Language Model (LLM) pre-training exhausts an ever growing compute
budget, yet recent research has demonstrated that careful document selection
enables comparable model quality with only a fraction of the FLOPs. Inspired by
efforts suggesting that domain-specific training document selection is in fact
an interpretable process [Gunasekar et al., 2023], as well as research showing
that instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et
al.,2023], we explore a promising direction for scalable general-domain
document selection; employing a prompted LLM as a document grader, we distill
quality labels into a classifier model, which is applied at scale to a large,
and already heavily-filtered, web-crawl-derived corpus autonomously. Following
the guidance of this classifier, we drop 75% of the corpus and train LLMs on
the remaining data. Results across multiple benchmarks show that: 1. Filtering
allows us to quality-match a model trained on the full corpus across diverse
benchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and
classifier models lead to better results that are less sensitive to the
labeler's prompt, 3. In-context learning helps to boost the performance of
less-capable labeling models. In all cases we use open-source datasets, models,
recipes, and evaluation frameworks, so that results can be reproduced by the
community.
</summary>
    <author>
      <name>Xiang Kong</name>
    </author>
    <author>
      <name>Tom Gunter</name>
    </author>
    <author>
      <name>Ruoming Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04643v1</id>
    <updated>2024-06-07T05:03:44Z</updated>
    <published>2024-06-07T05:03:44Z</published>
    <title>More Victories, Less Cooperation: Assessing Cicero's Diplomacy Play</title>
    <summary>  The boardgame Diplomacy is a challenging setting for communicative and
cooperative artificial intelligence. The most prominent communicative Diplomacy
AI, Cicero, has excellent strategic abilities, exceeding human players.
However, the best Diplomacy players master communication, not just tactics,
which is why the game has received attention as an AI challenge. This work
seeks to understand the degree to which Cicero succeeds at communication.
First, we annotate in-game communication with abstract meaning representation
to separate in-game tactics from general language. Second, we run two dozen
games with humans and Cicero, totaling over 200 human-player hours of
competition. While AI can consistently outplay human players, AI-Human
communication is still limited because of AI's difficulty with deception and
persuasion. This shows that Cicero relies on strategy and has not yet reached
the full promise of communicative and cooperative AI.
</summary>
    <author>
      <name>Wichayaporn Wongkamjan</name>
    </author>
    <author>
      <name>Feng Gu</name>
    </author>
    <author>
      <name>Yanze Wang</name>
    </author>
    <author>
      <name>Ulf Hermjakob</name>
    </author>
    <author>
      <name>Jonathan May</name>
    </author>
    <author>
      <name>Brandon M. Stewart</name>
    </author>
    <author>
      <name>Jonathan K. Kummerfeld</name>
    </author>
    <author>
      <name>Denis Peskoff</name>
    </author>
    <author>
      <name>Jordan Lee Boyd-Graber</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04669v1</id>
    <updated>2024-06-07T06:35:21Z</updated>
    <published>2024-06-07T06:35:21Z</published>
    <title>DiNeR: a Large Realistic Dataset for Evaluating Compositional
  Generalization</title>
    <summary>  Most of the existing compositional generalization datasets are
synthetically-generated, resulting in a lack of natural language variation.
While there have been recent attempts to introduce non-synthetic datasets for
compositional generalization, they suffer from either limited data scale or a
lack of diversity in the forms of combinations. To better investigate
compositional generalization with more linguistic phenomena and compositional
diversity, we propose the DIsh NamE Recognition (DiNeR) task and create a large
realistic Chinese dataset. Given a recipe instruction, models are required to
recognize the dish name composed of diverse combinations of food, actions, and
flavors. Our dataset consists of 3,811 dishes and 228,114 recipes, and involves
plenty of linguistic phenomena such as anaphora, omission and ambiguity. We
provide two strong baselines based on T5 and large language models (LLMs). This
work contributes a challenging task, baseline methods to tackle the task, and
insights into compositional generalization in the context of dish name
recognition. Code and data are available at https://github.com/Jumpy-pku/DiNeR.
</summary>
    <author>
      <name>Chengang Hu</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Yansong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2023 long paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04692v1</id>
    <updated>2024-06-07T07:04:10Z</updated>
    <published>2024-06-07T07:04:10Z</published>
    <title>Mixture-of-Agents Enhances Large Language Model Capabilities</title>
    <summary>  Recent advances in large language models (LLMs) demonstrate substantial
capabilities in natural language understanding and generation tasks. With the
growing number of LLMs, how to harness the collective expertise of multiple
LLMs is an exciting open direction. Toward this goal, we propose a new approach
that leverages the collective strengths of multiple LLMs through a
Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered
MoA architecture wherein each layer comprises multiple LLM agents. Each agent
takes all the outputs from agents in the previous layer as auxiliary
information in generating its response. MoA models achieves state-of-art
performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For
example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by
a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.
</summary>
    <author>
      <name>Junlin Wang</name>
    </author>
    <author>
      <name>Jue Wang</name>
    </author>
    <author>
      <name>Ben Athiwaratkun</name>
    </author>
    <author>
      <name>Ce Zhang</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04712v1</id>
    <updated>2024-06-07T07:45:38Z</updated>
    <published>2024-06-07T07:45:38Z</published>
    <title>AICoderEval: Improving AI Domain Code Generation of Large Language
  Models</title>
    <summary>  Automated code generation is a pivotal capability of large language models
(LLMs). However, assessing this capability in real-world scenarios remains
challenging. Previous methods focus more on low-level code generation, such as
model loading, instead of generating high-level codes catering for real-world
tasks, such as image-to-text, text classification, in various domains.
Therefore, we construct AICoderEval, a dataset focused on real-world tasks in
various domains based on HuggingFace, PyTorch, and TensorFlow, along with
comprehensive metrics for evaluation and enhancing LLMs' task-specific code
generation capability. AICoderEval contains test cases and complete programs
for automated evaluation of these tasks, covering domains such as natural
language processing, computer vision, and multimodal learning. To facilitate
research in this area, we open-source the AICoderEval dataset at
\url{https://huggingface.co/datasets/vixuowis/AICoderEval}. After that, we
propose CoderGen, an agent-based framework, to help LLMs generate codes related
to real-world tasks on the constructed AICoderEval. Moreover, we train a more
powerful task-specific code generation model, named AICoder, which is refined
on llama-3 based on AICoderEval. Our experiments demonstrate the effectiveness
of CoderGen in improving LLMs' task-specific code generation capability (by
12.00\% on pass@1 for original model and 9.50\% on pass@1 for ReAct Agent).
AICoder also outperforms current code generation LLMs, indicating the great
quality of the AICoderEval benchmark.
</summary>
    <author>
      <name>Yinghui Xia</name>
    </author>
    <author>
      <name>Yuyan Chen</name>
    </author>
    <author>
      <name>Tianyu Shi</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Jinsong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04744v1</id>
    <updated>2024-06-07T08:43:07Z</updated>
    <published>2024-06-07T08:43:07Z</published>
    <title>CRAG -- Comprehensive RAG Benchmark</title>
    <summary>  Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation on this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
&lt;=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
questions without any hallucination. CRAG also reveals much lower accuracy in
answering questions regarding facts with higher dynamism, lower popularity, or
higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of
participants and submissions within the first 50 days of the competition. We
commit to maintaining CRAG to serve research communities in advancing RAG
solutions and general QA solutions.
</summary>
    <author>
      <name>Xiao Yang</name>
    </author>
    <author>
      <name>Kai Sun</name>
    </author>
    <author>
      <name>Hao Xin</name>
    </author>
    <author>
      <name>Yushi Sun</name>
    </author>
    <author>
      <name>Nikita Bhalla</name>
    </author>
    <author>
      <name>Xiangsen Chen</name>
    </author>
    <author>
      <name>Sajal Choudhary</name>
    </author>
    <author>
      <name>Rongze Daniel Gui</name>
    </author>
    <author>
      <name>Ziran Will Jiang</name>
    </author>
    <author>
      <name>Ziyu Jiang</name>
    </author>
    <author>
      <name>Lingkun Kong</name>
    </author>
    <author>
      <name>Brian Moran</name>
    </author>
    <author>
      <name>Jiaqi Wang</name>
    </author>
    <author>
      <name>Yifan Ethan Xu</name>
    </author>
    <author>
      <name>An Yan</name>
    </author>
    <author>
      <name>Chenyu Yang</name>
    </author>
    <author>
      <name>Eting Yuan</name>
    </author>
    <author>
      <name>Hanwen Zha</name>
    </author>
    <author>
      <name>Nan Tang</name>
    </author>
    <author>
      <name>Lei Chen</name>
    </author>
    <author>
      <name>Nicolas Scheffer</name>
    </author>
    <author>
      <name>Yue Liu</name>
    </author>
    <author>
      <name>Nirav Shah</name>
    </author>
    <author>
      <name>Rakesh Wanga</name>
    </author>
    <author>
      <name>Anuj Kumar</name>
    </author>
    <author>
      <name>Wen-tau Yih</name>
    </author>
    <author>
      <name>Xin Luna Dong</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04752v1</id>
    <updated>2024-06-07T08:52:24Z</updated>
    <published>2024-06-07T08:52:24Z</published>
    <title>CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for
  Large Language Models</title>
    <summary>  Large language models (LLMs) are possessed of numerous beneficial
capabilities, yet their potential inclination harbors unpredictable risks that
may materialize in the future. We hence propose CRiskEval, a Chinese dataset
meticulously designed for gauging the risk proclivities inherent in LLMs such
as resource acquisition and malicious coordination, as part of efforts for
proactive preparedness. To curate CRiskEval, we define a new risk taxonomy with
7 types of frontier risks and 4 safety levels, including extremely
hazardous,moderately hazardous, neutral and safe. We follow the philosophy of
tendency evaluation to empirically measure the stated desire of LLMs via
fine-grained multiple-choice question answering. The dataset consists of 14,888
questions that simulate scenarios related to predefined 7 types of frontier
risks. Each question is accompanied with 4 answer choices that state opinions
or behavioral tendencies corresponding to the question. All answer choices are
manually annotated with one of the defined risk levels so that we can easily
build a fine-grained frontier risk profile for each assessed LLM. Extensive
evaluation with CRiskEval on a spectrum of prevalent Chinese LLMs has unveiled
a striking revelation: most models exhibit risk tendencies of more than 40%
(weighted tendency to the four risk levels). Furthermore, a subtle increase in
the model's inclination toward urgent self-sustainability, power seeking and
other dangerous goals becomes evident as the size of models increase. To
promote further research on the frontier risk evaluation of LLMs, we publicly
release our dataset at https://github.com/lingshi6565/Risk_eval.
</summary>
    <author>
      <name>Ling Shi</name>
    </author>
    <author>
      <name>Deyi Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04758v1</id>
    <updated>2024-06-07T08:58:29Z</updated>
    <published>2024-06-07T08:58:29Z</published>
    <title>Think out Loud: Emotion Deducing Explanation in Dialogues</title>
    <summary>  Humans convey emotions through daily dialogues, making emotion understanding
a crucial step of affective intelligence. To understand emotions in dialogues,
machines are asked to recognize the emotion for an utterance (Emotion
Recognition in Dialogues, ERD); based on the emotion, then find causal
utterances for the emotion (Emotion Cause Extraction in Dialogues, ECED). The
setting of the two tasks requires first ERD and then ECED, ignoring the mutual
complement between emotion and cause. To fix this, some new tasks are proposed
to extract them simultaneously. Although the current research on these tasks
has excellent achievements, simply identifying emotion-related factors by
classification modeling lacks realizing the specific thinking process of causes
stimulating the emotion in an explainable way. This thinking process especially
reflected in the reasoning ability of Large Language Models (LLMs) is
under-explored. To this end, we propose a new task "Emotion Deducing
Explanation in Dialogues" (EDEN). EDEN recognizes emotion and causes in an
explicitly thinking way. That is, models need to generate an explanation text,
which first summarizes the causes; analyzes the inner activities of the
speakers triggered by the causes using common sense; then guesses the emotion
accordingly. To support the study of EDEN, based on the existing resources in
ECED, we construct two EDEN datasets by human effort. We further evaluate
different models on EDEN and find that LLMs are more competent than
conventional PLMs. Besides, EDEN can help LLMs achieve better recognition of
emotions and causes, which explores a new research direction of explainable
emotion understanding in dialogues.
</summary>
    <author>
      <name>Jiangnan Li</name>
    </author>
    <author>
      <name>Zheng Lin</name>
    </author>
    <author>
      <name>Lanrui Wang</name>
    </author>
    <author>
      <name>Qingyi Si</name>
    </author>
    <author>
      <name>Yanan Cao</name>
    </author>
    <author>
      <name>Mo Yu</name>
    </author>
    <author>
      <name>Peng Fu</name>
    </author>
    <author>
      <name>Weiping Wang</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2406.04758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
